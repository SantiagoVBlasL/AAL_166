{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c259576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 20:40:46,293 - INFO - 3047307419.py:115 - Connectivity channels to be computed: ['Pearson_OMST_Weighted', 'dFC_AbsDiffMean', 'Lasso_VAR_Influence']\n",
      "2025-05-29 20:40:46,294 - INFO - 3047307419.py:116 - Total number of channels: 3\n",
      "2025-05-29 20:40:46,298 - INFO - 3047307419.py:624 - Main process RAM at start: 239.98 MB\n",
      "2025-05-29 20:40:46,298 - INFO - 3047307419.py:625 - --- Starting fMRI Connectivity Pipeline (Luppi et al. 2024 informed) ---\n",
      "2025-05-29 20:40:46,299 - INFO - 3047307419.py:626 - --- Output Directory: AAL3_131_fmri_tensor_LuppiOMST_1lag_NeuroEnhanced_v6.1 ---\n",
      "2025-05-29 20:40:46,299 - INFO - 3047307419.py:627 - --- N_ROIS_EXPECTED (final for connectivity): 131 ---\n",
      "2025-05-29 20:40:46,299 - INFO - 3047307419.py:628 - --- TARGET_LEN_TS (homogenized time series length): 140 ---\n",
      "2025-05-29 20:40:46,300 - INFO - 3047307419.py:125 - --- Starting Subject Metadata Loading and QC Integration ---\n",
      "2025-05-29 20:40:46,308 - INFO - 3047307419.py:136 - Loaded main metadata from /home/diego/Escritorio/AAL3/SubjctsDataAndTests_Schaefer2018_400Parcels_17Networks.csv. Shape: (434, 24)\n",
      "2025-05-29 20:40:46,311 - CRITICAL - 3047307419.py:200 - Unexpected error during metadata loading/QC integration: 'SubjectID'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/diego/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'SubjectID'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1237813/3047307419.py\", line 144, in load_metadata\n",
      "    qc_df['SubjectID'] = qc_df['SubjectID'].astype(str).str.strip()\n",
      "                         ~~~~~^^^^^^^^^^^^^\n",
      "  File \"/home/diego/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diego/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'SubjectID'\n",
      "2025-05-29 20:40:46,316 - CRITICAL - 3047307419.py:636 - Metadata loading failed or no subjects passed QC. Aborting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: The 'omst' package (topological_filtering_networks) is not installed. Please install it: pip install git+https://github.com/stdimitr/topological_filtering_networks.git\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.glm.first_level import spm_hrf, glover_hrf\n",
    "from scipy.signal import butter, filtfilt, deconvolve\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "\n",
    "# Import for OMST - ensure 'topological_filtering_networks' is installed\n",
    "try:\n",
    "    from omst.omst import orthogonal_minimum_spanning_tree\n",
    "except ImportError:\n",
    "    print(\"ERROR: The 'omst' package (topological_filtering_networks) is not installed. Please install it: pip install git+https://github.com/stdimitr/topological_filtering_networks.git\")\n",
    "    orthogonal_minimum_spanning_tree = None # Placeholder if not installed\n",
    "\n",
    "\n",
    "# --- 0. Global Configuration and Constants ---\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Neuroscientific Context & Recommendations Summary (Updated for Luppi et al. 2024) ---\n",
    "# This script aims to generate multi-modal brain connectivity matrices from fMRI time series\n",
    "# using the AAL3 atlas (166 ROIs, further refined by QC).\n",
    "# These matrices can serve as features for machine learning models to classify neurological conditions.\n",
    "#\n",
    "# CRITICAL FOR THESIS: This script should be run AFTER your QC pipeline.\n",
    "# The `load_metadata` function is designed to read the output of your QC script\n",
    "# to ensure only quality-controlled subjects are processed.\n",
    "#\n",
    "# KEY UPDATE (Luppi et al., 2024, Nature Communications):\n",
    "# This version incorporates recommendations for constructing static functional connectivity (sFC)\n",
    "# matrices that demonstrated high consistency and reliability in a systematic evaluation\n",
    "# of 768 pipelines. The core recommended sFC pipeline is:\n",
    "# 1. Pearson Correlation\n",
    "# 2. Fisher R-to-Z transformation\n",
    "# 3. Orthogonal Minimum Spanning Tree (OMST) filtering (on absolute Fisher-Z values)\n",
    "# 4. Using weighted (not binary) matrices.\n",
    "# This \"Pearson_OMST_Weighted\" channel is now the primary sFC modality.\n",
    "\n",
    "### NEURO-ENHANCEMENT: Summary of evidence-based recommendations:\n",
    "# | Aspect                      | Recommended Practice                                      | Rationale / References                                                                                                                                  |\n",
    "# |-----------------------------|-----------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "# | Data Input                  | Use data that has passed rigorous QC                    | Ensures reliability and validity of connectivity findings. (Your QC script output is key).                                                              |\n",
    "# | Atlas Consideration         | AAL3 (e.g., 131 ROIs after your QC filtering)           | Document the final ROI set used after all filtering (AAL3 base, missing ROIs, small volume ROIs).                                                       |\n",
    "# | **Static FC (Primary)** | **Pearson -> Fisher-Z -> abs -> OMST -> Weighted** | **One of 9 optimal pipelines from Luppi et al. (2024) for consistent functional connectomics.** |\n",
    "# | Band-pass Filtering         | Yes, 0.01–0.08 Hz before connectivity                   | Eliminates scanner drift (<0.01 Hz) & physiological noise (>0.08 Hz). (PMID: 17266107)                                                                   |\n",
    "# | k in MI (k-NN) (Optional)   | k = 5 (e.g., KSG estimator)                             | k≈3–10 common; k=5 offers good bias-variance balance for N~171+ TPs. (PMID: 15042233)                                                                   |\n",
    "# | Max VAR Order (Lag)         | Lag 1 preferred, especially for typical fMRI TS         | For TR~2–3s, optimal order often 1. Higher orders w/ short TS & many ROIs risk overfitting. (PMID: 21833123)                                           |\n",
    "# | HRF Deconvolution           | Optional, for neuronal-level VAR. Use with caution.     | Attempts to recover neural signal from BOLD. Can improve EC interpretability but is sensitive to noise & HRF model. Validate carefully. (PMID: 17329248)|\n",
    "# | Time Series Length          | Consistent length for analysis (e.g., `TARGET_LEN_TS`)    | Ensure `TARGET_LEN_TS` is appropriate for subjects passing QC.                                                                                          |\n",
    "\n",
    "\n",
    "# --- Configurable Parameters ---\n",
    "BASE_PATH_AAL3 = Path('/home/diego/Escritorio/AAL3')\n",
    "QC_OUTPUT_DIR = BASE_PATH_AAL3 / 'qc_outputs_doctoral_v3.2_aal3_shrinkage_flexible_thresh_fix' # Points to v3.2 QC output\n",
    "SUBJECT_METADATA_CSV_PATH = BASE_PATH_AAL3 / 'SubjctsDataAndTests_Schaefer2018_400Parcels_17Networks.csv'\n",
    "QC_REPORT_CSV_PATH = QC_OUTPUT_DIR / 'report_qc_final_with_discard_flags_v3.2.csv'\n",
    "ROI_SIGNALS_DIR_PATH_AAL3 = BASE_PATH_AAL3 / 'ROISignals_AAL3_NiftiPreprocessedAllBatchesNorm'\n",
    "ROI_FILENAME_TEMPLATE = 'ROISignals_{subject_id}.mat'\n",
    "\n",
    "TR_SECONDS = 3.0\n",
    "LOW_CUT_HZ = 0.01\n",
    "HIGH_CUT_HZ = 0.08\n",
    "FILTER_ORDER = 2\n",
    "\n",
    "N_ROIS_EXPECTED = 131 # Based on your QC script v3.2 output (ROIs_for_MV = 131)\n",
    "TARGET_LEN_TS = 140   # Based on your QC script v3.2 (TP_THRESHOLD_VALUE = 140)\n",
    "\n",
    "N_NEIGHBORS_MI = 5\n",
    "DFC_WIN_POINTS = 20\n",
    "DFC_STEP = 10\n",
    "LASSO_VAR_MAX_LAG = 1\n",
    "APPLY_HRF_DECONVOLUTION = False\n",
    "HRF_MODEL = 'glover'\n",
    "LASSO_MAX_ITER = 2500\n",
    "LASSO_TOL = 1e-4\n",
    "\n",
    "deconv_str = \"_deconv\" if APPLY_HRF_DECONVOLUTION else \"\"\n",
    "OUTPUT_CONNECTIVITY_DIR_NAME = f\"AAL3_{N_ROIS_EXPECTED}_fmri_tensor_LuppiOMST_{LASSO_VAR_MAX_LAG}lag{deconv_str}_NeuroEnhanced_v6.1\" # Version increment\n",
    "\n",
    "POSSIBLE_ROI_KEYS = [\"signals\", \"ROISignals\", \"roi_signals\", \"ROIsignals_AAL3\", \"AAL3_signals\", \"roi_ts\"]\n",
    "\n",
    "# --- Define which connectivity channels to compute ---\n",
    "PEARSON_OMST_CHANNEL_NAME = \"Pearson_OMST_Weighted\" # Luppi et al. (2024) recommended\n",
    "\n",
    "# Flags to control optional channels, as per user instruction for thesis \"core\"\n",
    "USE_MI_CHANNEL_FOR_THESIS = False\n",
    "USE_DFC_CHANNEL_FOR_THESIS = True\n",
    "USE_LASSO_VAR_FOR_THESIS = True\n",
    "\n",
    "# Dynamically build the list of channel names to be processed\n",
    "CONNECTIVITY_CHANNEL_NAMES = [PEARSON_OMST_CHANNEL_NAME] # Start with the primary channel\n",
    "if USE_MI_CHANNEL_FOR_THESIS:\n",
    "    CONNECTIVITY_CHANNEL_NAMES.append(\"MI_KNN\")\n",
    "if USE_DFC_CHANNEL_FOR_THESIS:\n",
    "    CONNECTIVITY_CHANNEL_NAMES.append(\"dFC_AbsDiffMean\")\n",
    "if USE_LASSO_VAR_FOR_THESIS:\n",
    "    var_channel_suffix_dynamic = \"_Deconv_Influence\" if APPLY_HRF_DECONVOLUTION else \"_Influence\"\n",
    "    CONNECTIVITY_CHANNEL_NAMES.append(f\"Lasso_VAR{var_channel_suffix_dynamic}\")\n",
    "\n",
    "N_CHANNELS = len(CONNECTIVITY_CHANNEL_NAMES)\n",
    "logger.info(f\"Connectivity channels to be computed: {CONNECTIVITY_CHANNEL_NAMES}\")\n",
    "logger.info(f\"Total number of channels: {N_CHANNELS}\")\n",
    "\n",
    "\n",
    "# --- 1. Subject Metadata Loading and Merging ---\n",
    "def load_metadata(\n",
    "    subject_meta_csv_path: Path,\n",
    "    qc_report_csv_path: Path\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Loads subject metadata, merges with QC results, and filters subjects.\"\"\"\n",
    "    logger.info(\"--- Starting Subject Metadata Loading and QC Integration ---\")\n",
    "    try:\n",
    "        if not subject_meta_csv_path.exists():\n",
    "            logger.critical(f\"Subject metadata CSV file NOT found: {subject_meta_csv_path}\")\n",
    "            return None\n",
    "        if not qc_report_csv_path.exists():\n",
    "            logger.critical(f\"QC report CSV file NOT found: {qc_report_csv_path}\")\n",
    "            return None\n",
    "\n",
    "        subjects_db_df = pd.read_csv(subject_meta_csv_path)\n",
    "        subjects_db_df['SubjectID'] = subjects_db_df['SubjectID'].astype(str).str.strip()\n",
    "        logger.info(f\"Loaded main metadata from {subject_meta_csv_path}. Shape: {subjects_db_df.shape}\")\n",
    "        if 'SubjectID' not in subjects_db_df.columns:\n",
    "            logger.critical(\"Column 'SubjectID' missing in main metadata CSV.\")\n",
    "            return None\n",
    "        if 'ResearchGroup' not in subjects_db_df.columns:\n",
    "            logger.warning(\"Column 'ResearchGroup' missing in main metadata CSV. May be needed for downstream VAE tasks.\")\n",
    "\n",
    "        qc_df = pd.read_csv(qc_report_csv_path)\n",
    "        qc_df['SubjectID'] = qc_df['SubjectID'].astype(str).str.strip()\n",
    "        logger.info(f\"Loaded QC report from {qc_report_csv_path}. Shape: {qc_df.shape}\")\n",
    "        # Ensure 'TimePoints' from QC script (not 'Timepoints_final' from previous version of this script)\n",
    "        if not all(col in qc_df.columns for col in ['SubjectID', 'ToDiscard_Overall', 'TimePoints']):\n",
    "            logger.critical(\"Essential columns ('SubjectID', 'ToDiscard_Overall', 'TimePoints') missing in QC report CSV.\")\n",
    "            return None\n",
    "\n",
    "        merged_df = pd.merge(subjects_db_df, qc_df, on='SubjectID', how='inner', suffixes=('_meta', '_qc'))\n",
    "        \n",
    "        # Determine the definitive 'Timepoints' column, prioritizing from QC\n",
    "        if 'TimePoints_qc' in merged_df.columns: # If QC report had 'TimePoints' and it got suffixed\n",
    "            merged_df['Timepoints_final_for_script'] = merged_df['TimePoints_qc']\n",
    "        elif 'TimePoints' in merged_df.columns: # If QC report had 'TimePoints' and it was not suffixed (or was the only one)\n",
    "             merged_df['Timepoints_final_for_script'] = merged_df['TimePoints']\n",
    "        else:\n",
    "            logger.critical(\"Definitive 'TimePoints' column from QC report could not be identified after merge.\")\n",
    "            return None\n",
    "        \n",
    "        merged_df['Timepoints_final_for_script'] = pd.to_numeric(merged_df['Timepoints_final_for_script'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        initial_subject_count = len(merged_df)\n",
    "        subjects_passing_qc_df = merged_df[merged_df['ToDiscard_Overall'] == False].copy()\n",
    "        num_discarded = initial_subject_count - len(subjects_passing_qc_df)\n",
    "        logger.info(f\"Total subjects after merge: {initial_subject_count}\")\n",
    "        logger.info(f\"Subjects discarded based on QC ('ToDiscard_Overall' == True): {num_discarded}\")\n",
    "        logger.info(f\"Subjects passing QC and to be processed: {len(subjects_passing_qc_df)}\")\n",
    "\n",
    "        if subjects_passing_qc_df.empty:\n",
    "            logger.warning(\"No subjects passed QC. Check your QC criteria and report.\")\n",
    "            return None\n",
    "            \n",
    "        min_tp_after_qc = subjects_passing_qc_df['Timepoints_final_for_script'].min()\n",
    "        max_tp_after_qc = subjects_passing_qc_df['Timepoints_final_for_script'].max()\n",
    "        logger.info(f\"Timepoints for subjects passing QC (from QC report): Min={min_tp_after_qc}, Max={max_tp_after_qc}.\")\n",
    "        logger.info(f\"These will be homogenized to TARGET_LEN_TS = {TARGET_LEN_TS} for connectivity calculation.\")\n",
    "\n",
    "        final_cols_to_keep = ['SubjectID', 'Timepoints_final_for_script']\n",
    "        if 'ResearchGroup' in subjects_passing_qc_df.columns:\n",
    "            final_cols_to_keep.append('ResearchGroup')\n",
    "        else:\n",
    "            logger.warning(\"Creating placeholder 'ResearchGroup' column as it was not found.\")\n",
    "            subjects_passing_qc_df['ResearchGroup'] = 'Unknown' # Ensure it exists\n",
    "            final_cols_to_keep.append('ResearchGroup')\n",
    "        \n",
    "        # Rename 'Timepoints_final_for_script' to 'Timepoints' for consistency downstream in this script\n",
    "        subjects_passing_qc_df.rename(columns={'Timepoints_final_for_script': 'Timepoints'}, inplace=True)\n",
    "        \n",
    "        return subjects_passing_qc_df[final_cols_to_keep]\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logger.critical(f\"CRITICAL Error loading CSV files: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        logger.critical(f\"Value error in metadata processing: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Unexpected error during metadata loading/QC integration: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "# --- 2. Time Series Loading and Preprocessing Functions ---\n",
    "def _load_signals_from_mat(mat_path: Path, possible_keys: List[str]) -> Optional[np.ndarray]:\n",
    "    try:\n",
    "        data = sio.loadmat(mat_path)\n",
    "    except Exception as e_load:\n",
    "        logger.error(f\"Could not load .mat file: {mat_path}. Error: {e_load}\")\n",
    "        return None\n",
    "    for key in possible_keys:\n",
    "        if key in data and isinstance(data[key], np.ndarray) and data[key].ndim >= 2:\n",
    "            logger.debug(f\"Found signals under key '{key}' in {mat_path}.\")\n",
    "            return data[key]\n",
    "    logger.warning(f\"No valid signal keys {possible_keys} found in {mat_path}. Keys present: {list(data.keys())}\")\n",
    "    return None\n",
    "\n",
    "def _orient_and_validate_signals(sigs: np.ndarray, n_rois_expected_val: int, subject_id: str) -> Optional[np.ndarray]:\n",
    "    if sigs.ndim != 2:\n",
    "        logger.warning(f\"S {subject_id}: Signal matrix has incorrect dimensions {sigs.ndim} (expected 2).\")\n",
    "        return None\n",
    "    if sigs.shape[0] == n_rois_expected_val and sigs.shape[1] != n_rois_expected_val:\n",
    "        logger.info(f\"S {subject_id}: Transposing matrix from {sigs.shape} to ({sigs.shape[1]}, {sigs.shape[0]}).\")\n",
    "        sigs = sigs.T\n",
    "    elif sigs.shape[1] == n_rois_expected_val and sigs.shape[0] != n_rois_expected_val:\n",
    "        pass \n",
    "    elif sigs.shape[0] == n_rois_expected_val and sigs.shape[1] == n_rois_expected_val:\n",
    "         logger.warning(f\"S {subject_id}: Signal matrix is square ({sigs.shape}) and matches N_ROIS_EXPECTED. Assuming [Timepoints, ROIs]. Verify assumption if TPs can also be {n_rois_expected_val}.\")\n",
    "    else: \n",
    "        logger.warning(f\"S {subject_id}: Neither dimension of signal matrix ({sigs.shape}) matches N_ROIS_EXPECTED ({n_rois_expected_val}).\")\n",
    "        return None\n",
    "    if sigs.shape[1] != n_rois_expected_val: \n",
    "        logger.warning(f\"S {subject_id}: After orientation, ROI count ({sigs.shape[1]}) != N_ROIS_EXPECTED ({n_rois_expected_val}).\")\n",
    "        return None\n",
    "    return sigs\n",
    "\n",
    "def _bandpass_filter_signals(sigs: np.ndarray, lowcut: float, highcut: float, fs: float, order: int, subject_id: str) -> np.ndarray:\n",
    "    nyquist_freq = 0.5 * fs\n",
    "    low_norm = lowcut / nyquist_freq\n",
    "    high_norm = highcut / nyquist_freq\n",
    "    if not (0 < low_norm < 1 and 0 < high_norm < 1 and low_norm < high_norm):\n",
    "        logger.error(f\"S {subject_id}: Invalid critical frequencies for filter. Skipping filtering.\")\n",
    "        return sigs\n",
    "    try:\n",
    "        b, a = butter(order, [low_norm, high_norm], btype='band', analog=False)\n",
    "        filtered_sigs = np.zeros_like(sigs)\n",
    "        padlen_required = 3 * max(len(a), len(b))\n",
    "        for i in range(sigs.shape[1]):\n",
    "            roi_signal = sigs[:, i]\n",
    "            if np.all(np.isclose(roi_signal, roi_signal[0] if len(roi_signal)>0 else 0)):\n",
    "                logger.debug(f\"S {subject_id}, ROI {i}: Signal constant. Skipping filter.\")\n",
    "                filtered_sigs[:, i] = roi_signal\n",
    "            elif len(roi_signal) <= padlen_required :\n",
    "                logger.warning(f\"S {subject_id}, ROI {i}: Signal too short ({len(roi_signal)} pts) for filtfilt. Skipping filter.\")\n",
    "                filtered_sigs[:, i] = roi_signal\n",
    "            else:\n",
    "                filtered_sigs[:, i] = filtfilt(b, a, roi_signal)\n",
    "        return filtered_sigs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"S {subject_id}: Error during bandpass filtering: {e}. Returning original.\", exc_info=False)\n",
    "        return sigs\n",
    "\n",
    "def _hrf_deconvolution(sigs: np.ndarray, tr: float, hrf_model_type: str, subject_id: str) -> np.ndarray:\n",
    "    logger.info(f\"S {subject_id}: Attempting HRF deconvolution ({hrf_model_type} model, TR={tr}s).\")\n",
    "    if hrf_model_type == 'glover': hrf_kernel = glover_hrf(tr, oversampling=1)\n",
    "    elif hrf_model_type == 'spm': hrf_kernel = spm_hrf(tr, oversampling=1)\n",
    "    else: logger.error(f\"S {subject_id}: Unknown HRF model '{hrf_model_type}'. Skipping deconv.\"); return sigs\n",
    "    if len(hrf_kernel) == 0 or np.all(np.isclose(hrf_kernel, 0)):\n",
    "        logger.error(f\"S {subject_id}: HRF kernel empty/zero for {hrf_model_type}. Skipping deconv.\"); return sigs\n",
    "    deconvolved_sigs = np.zeros_like(sigs)\n",
    "    for i in range(sigs.shape[1]):\n",
    "        signal_roi = sigs[:, i]\n",
    "        if len(signal_roi) < len(hrf_kernel):\n",
    "            logger.warning(f\"S {subject_id}, ROI {i}: Signal shorter than HRF. Skipping deconv.\")\n",
    "            deconvolved_sigs[:, i] = signal_roi; continue\n",
    "        try:\n",
    "            quotient, _ = deconvolve(signal_roi, hrf_kernel)\n",
    "            if len(quotient) < sigs.shape[0]:\n",
    "                deconvolved_sigs[:, i] = np.concatenate([quotient, np.zeros(sigs.shape[0] - len(quotient))])\n",
    "            else: deconvolved_sigs[:, i] = quotient[:sigs.shape[0]]\n",
    "        except Exception as e_deconv:\n",
    "            logger.error(f\"S {subject_id}, ROI {i}: Deconvolution failed: {e_deconv}. Using original.\", exc_info=False)\n",
    "            deconvolved_sigs[:, i] = signal_roi\n",
    "    logger.info(f\"S {subject_id}: HRF deconvolution attempt finished.\")\n",
    "    return deconvolved_sigs\n",
    "\n",
    "def _preprocess_time_series(\n",
    "    sigs: np.ndarray, target_len_ts_val: int, n_rois_expected_val: int,\n",
    "    subject_id: str, lasso_var_max_lag_val: int,\n",
    "    tr_seconds_val: float, low_cut_val: float, high_cut_val: float, filter_order_val: int,\n",
    "    apply_hrf_deconv_val: bool, hrf_model_type_val: str\n",
    ") -> Optional[np.ndarray]:\n",
    "    original_length = sigs.shape[0]\n",
    "    fs = 1.0 / tr_seconds_val\n",
    "    logger.info(f\"S {subject_id}: Preprocessing. Original TPs: {original_length}, TR: {tr_seconds_val}s, Target TPs: {target_len_ts_val}.\")\n",
    "    sigs_processed = _bandpass_filter_signals(sigs, low_cut_val, high_cut_val, fs, filter_order_val, subject_id)\n",
    "    if apply_hrf_deconv_val:\n",
    "        sigs_processed = _hrf_deconvolution(sigs_processed, tr_seconds_val, hrf_model_type_val, subject_id)\n",
    "        if np.isnan(sigs_processed).any() or np.isinf(sigs_processed).any():\n",
    "            logger.warning(f\"S {subject_id}: NaNs/Infs after HRF deconvolution. Cleaning.\")\n",
    "            sigs_processed = np.nan_to_num(sigs_processed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    min_len_for_var = lasso_var_max_lag_val + 10\n",
    "    min_len_for_dfc = DFC_WIN_POINTS if DFC_WIN_POINTS > 0 else 5\n",
    "    min_overall_len = max(5, min_len_for_var, min_len_for_dfc)\n",
    "    if sigs_processed.shape[0] < min_overall_len:\n",
    "        logger.warning(f\"S {subject_id}: TPs after processing ({sigs_processed.shape[0]}) < min required ({min_overall_len}). Skipping.\")\n",
    "        return None\n",
    "    if np.isnan(sigs_processed).any():\n",
    "        logger.warning(f\"S {subject_id}: NaNs before scaling. Filling with 0.0.\")\n",
    "        sigs_processed = np.nan_to_num(sigs_processed, nan=0.0)\n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        sigs_normalized = scaler.fit_transform(sigs_processed)\n",
    "        if np.isnan(sigs_normalized).any():\n",
    "            logger.warning(f\"S {subject_id}: NaNs after StandardScaler. Filling with 0.0.\")\n",
    "            sigs_normalized = np.nan_to_num(sigs_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    except ValueError: \n",
    "        logger.warning(f\"S {subject_id}: StandardScaler failed. Attempting column-wise scaling.\")\n",
    "        sigs_normalized = np.zeros_like(sigs_processed, dtype=np.float32)\n",
    "        for i in range(sigs_processed.shape[1]):\n",
    "            col_data = sigs_processed[:, i].reshape(-1,1)\n",
    "            if np.std(col_data) > 1e-9:\n",
    "                try: sigs_normalized[:, i] = StandardScaler().fit_transform(col_data).flatten()\n",
    "                except: sigs_normalized[:, i] = 0.0\n",
    "            else: sigs_normalized[:, i] = 0.0\n",
    "        if np.isnan(sigs_normalized).any():\n",
    "            sigs_normalized = np.nan_to_num(sigs_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    current_length = sigs_normalized.shape[0]\n",
    "    if current_length != target_len_ts_val:\n",
    "        logger.info(f\"S {subject_id}: Homogenizing length from {current_length} to {target_len_ts_val}.\")\n",
    "        if current_length < target_len_ts_val:\n",
    "            padding = np.zeros((target_len_ts_val - current_length, n_rois_expected_val), dtype=np.float32)\n",
    "            sigs_homogenized = np.vstack((sigs_normalized, padding))\n",
    "        else:\n",
    "            sigs_homogenized = sigs_normalized[:target_len_ts_val, :]\n",
    "    else:\n",
    "        sigs_homogenized = sigs_normalized\n",
    "    return sigs_homogenized.astype(np.float32)\n",
    "\n",
    "def load_and_preprocess_single_subject_series(\n",
    "    subject_id: str, n_rois_expected_val: int, target_len_ts_val: int,\n",
    "    current_roi_signals_dir_path: Path, current_roi_filename_template: str,\n",
    "    possible_roi_keys_list: List[str], lasso_var_max_lag_val: int,\n",
    "    tr_seconds_val: float, low_cut_val: float, high_cut_val: float, filter_order_val: int,\n",
    "    apply_hrf_deconv_val: bool, hrf_model_type_val: str\n",
    ") -> Tuple[Optional[np.ndarray], str, bool]:\n",
    "    mat_path = current_roi_signals_dir_path / current_roi_filename_template.format(subject_id=subject_id)\n",
    "    if not mat_path.exists(): return None, f\"MAT file not found: {mat_path}\", False\n",
    "    try:\n",
    "        loaded_sigs_raw = _load_signals_from_mat(mat_path, possible_roi_keys_list)\n",
    "        if loaded_sigs_raw is None: return None, f\"No valid signal keys or load error in {mat_path}\", False\n",
    "        loaded_sigs_raw = np.asarray(loaded_sigs_raw, dtype=np.float64)\n",
    "        sigs_oriented = _orient_and_validate_signals(loaded_sigs_raw, n_rois_expected_val, subject_id)\n",
    "        del loaded_sigs_raw; gc.collect()\n",
    "        if sigs_oriented is None: return None, f\"Incorrect shape or validation failed for S {subject_id}.\", False\n",
    "        original_tp_count = sigs_oriented.shape[0]\n",
    "        sigs_processed = _preprocess_time_series(\n",
    "            sigs_oriented, target_len_ts_val, n_rois_expected_val, subject_id, lasso_var_max_lag_val,\n",
    "            tr_seconds_val, low_cut_val, high_cut_val, filter_order_val,\n",
    "            apply_hrf_deconv_val, hrf_model_type_val\n",
    "        )\n",
    "        del sigs_oriented; gc.collect()\n",
    "        if sigs_processed is None: return None, f\"Preprocessing failed for S {subject_id}. Original TPs: {original_tp_count}\", False\n",
    "        final_shape_str = f\"({sigs_processed.shape[0]}, {sigs_processed.shape[1]})\"\n",
    "        return sigs_processed, f\"OK. Original TPs: {original_tp_count}, final shape: {final_shape_str}\", True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unhandled exception during load_and_preprocess for S {subject_id} ({mat_path}): {e}\", exc_info=True)\n",
    "        return None, f\"Exception processing {mat_path}: {str(e)}\", False\n",
    "\n",
    "# --- 3. Connectivity Calculation Functions ---\n",
    "\n",
    "def fisher_r_to_z(r_matrix: np.ndarray, eps: float = 1e-7) -> np.ndarray:\n",
    "    r_clean = np.nan_to_num(r_matrix.astype(np.float32), nan=0.0)\n",
    "    r_clipped = np.clip(r_clean, -1.0 + eps, 1.0 - eps)\n",
    "    z_matrix = np.arctanh(r_clipped)\n",
    "    np.fill_diagonal(z_matrix, 0.0)\n",
    "    return z_matrix.astype(np.float32)\n",
    "\n",
    "def calculate_pearson_omst(ts_subject: np.ndarray, sid: str) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates Pearson correlation, applies Fisher-Z, takes absolute values for weights,\n",
    "    and then applies OMST filtering. Returns a weighted (N_ROIS x N_ROIS) matrix.\n",
    "    \"\"\"\n",
    "    if orthogonal_minimum_spanning_tree is None:\n",
    "        logger.error(f\"Pearson_OMST (S {sid}): OMST package not available. Cannot calculate this modality.\")\n",
    "        return None\n",
    "    if ts_subject.shape[0] < 2:\n",
    "        logger.warning(f\"Pearson_OMST (S {sid}): Insufficient timepoints ({ts_subject.shape[0]} < 2).\")\n",
    "        return None\n",
    "    try:\n",
    "        corr_matrix = np.corrcoef(ts_subject, rowvar=False).astype(np.float32)\n",
    "        if corr_matrix.ndim == 0: \n",
    "            logger.warning(f\"Pearson_OMST (S {sid}): Correlation resulted in a scalar. Input shape: {ts_subject.shape}. Returning zero matrix.\")\n",
    "            return np.zeros((ts_subject.shape[1], ts_subject.shape[1]), dtype=np.float32) if ts_subject.shape[1] > 0 else None\n",
    "        np.fill_diagonal(corr_matrix, 0.0)\n",
    "        z_transformed_matrix = fisher_r_to_z(corr_matrix)\n",
    "        weights_for_omst = np.abs(z_transformed_matrix)\n",
    "        np.fill_diagonal(weights_for_omst, 0.0) \n",
    "        if np.all(np.isclose(weights_for_omst, 0)):\n",
    "             logger.warning(f\"Pearson_OMST (S {sid}): All weights for OMST are zero after abs(Fisher-Z). Returning zero matrix.\")\n",
    "             return weights_for_omst.astype(np.float32)\n",
    "        w_omst = orthogonal_minimum_spanning_tree(weights_for_omst, cost_function='efficiency_cost')\n",
    "        np.fill_diagonal(w_omst, 0.0) # Ensure diagonal is zero after OMST\n",
    "        logger.info(f\"Pearson_OMST (S {sid}): Successfully calculated. Matrix density: {np.count_nonzero(w_omst) / w_omst.size:.4f}\")\n",
    "        return w_omst.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating Pearson_OMST connectivity for S {sid}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def calculate_mi_knn_connectivity(ts_subject: np.ndarray, n_neighbors_val: int, sid: str) -> Optional[np.ndarray]:\n",
    "    n_tp, n_rois = ts_subject.shape\n",
    "    if n_tp == 0: logger.warning(f\"MI_KNN (S {sid}): 0 TPs.\"); return None\n",
    "    if n_tp <= n_neighbors_val:\n",
    "        logger.warning(f\"MI_KNN (S {sid}): TPs ({n_tp}) <= n_neighbors ({n_neighbors_val}). Skipping.\")\n",
    "        return None\n",
    "    mi_matrix = np.zeros((n_rois, n_rois), dtype=np.float32)\n",
    "    for i in range(n_rois):\n",
    "        for j in range(i + 1, n_rois):\n",
    "            try:\n",
    "                X_i_reshaped = ts_subject[:, i].reshape(-1, 1)\n",
    "                y_j = ts_subject[:, j]\n",
    "                mi_val = mutual_info_regression(X_i_reshaped, y_j, n_neighbors=n_neighbors_val, random_state=42, discrete_features=False)[0]\n",
    "                mi_matrix[i, j] = mi_matrix[j, i] = mi_val\n",
    "            except Exception as e:\n",
    "                logger.error(f\"MI_KNN (S {sid}), pair ({i},{j}): {e}\", exc_info=False)\n",
    "                mi_matrix[i, j] = mi_matrix[j, i] = 0.0\n",
    "    return mi_matrix\n",
    "\n",
    "def calculate_custom_dfc_abs_diff_mean(ts_subject: np.ndarray, win_points_val: int, step_val: int, sid: str) -> Optional[np.ndarray]:\n",
    "    n_tp, n_rois = ts_subject.shape\n",
    "    if n_tp < win_points_val: logger.warning(f\"dFC (S {sid}): TPs < window. Skipping.\"); return None\n",
    "    num_windows = (n_tp - win_points_val) // step_val + 1\n",
    "    if num_windows < 2: logger.warning(f\"dFC (S {sid}): <2 windows. Skipping.\"); return None\n",
    "    sum_abs_diff_matrix = np.zeros((n_rois, n_rois), dtype=np.float64)\n",
    "    n_diffs_calculated = 0\n",
    "    prev_corr_matrix_abs: Optional[np.ndarray] = None\n",
    "    for idx in range(num_windows):\n",
    "        start_idx, end_idx = idx * step_val, start_idx + win_points_val\n",
    "        window_ts = ts_subject[start_idx:end_idx, :]\n",
    "        if window_ts.shape[0] < 2: continue\n",
    "        try:\n",
    "            corr_matrix_window = np.corrcoef(window_ts, rowvar=False)\n",
    "            if corr_matrix_window.ndim < 2 or corr_matrix_window.shape != (n_rois, n_rois):\n",
    "                corr_matrix_window = np.full((n_rois, n_rois), 0.0, dtype=np.float32)\n",
    "            else:\n",
    "                corr_matrix_window = np.nan_to_num(corr_matrix_window.astype(np.float32), nan=0.0)\n",
    "            current_corr_matrix_abs = np.abs(corr_matrix_window)\n",
    "            np.fill_diagonal(current_corr_matrix_abs, 0)\n",
    "            if prev_corr_matrix_abs is not None:\n",
    "                sum_abs_diff_matrix += np.abs(current_corr_matrix_abs - prev_corr_matrix_abs)\n",
    "                n_diffs_calculated += 1\n",
    "            prev_corr_matrix_abs = current_corr_matrix_abs\n",
    "        except Exception as e: logger.error(f\"dFC (S {sid}), Window {idx}: Error: {e}\")\n",
    "    if n_diffs_calculated == 0: logger.warning(f\"dFC (S {sid}): No valid diffs. Returning None.\"); return None\n",
    "    mean_abs_diff_matrix = (sum_abs_diff_matrix / n_diffs_calculated).astype(np.float32)\n",
    "    np.fill_diagonal(mean_abs_diff_matrix, 0)\n",
    "    return mean_abs_diff_matrix\n",
    "\n",
    "def calculate_lasso_var_connectivity_matrix(ts: np.ndarray, max_lag_val: int, sid: str) -> Optional[np.ndarray]:\n",
    "    n_tp, n_rois = ts.shape; p = max_lag_val\n",
    "    if n_tp <= p: logger.warning(f\"LassoVAR (S {sid}): TPs <= lag. Skipping.\"); return None\n",
    "    if n_tp < p * n_rois / 5 : logger.warning(f\"LassoVAR (S {sid}): TPs very low for ROIs*lag. Model might be unstable.\")\n",
    "    elif n_tp < p * n_rois: logger.info(f\"LassoVAR (S {sid}): TPs < ROIs*lag. Relying on Lasso.\")\n",
    "    Y_data = ts[p:, :].astype(np.float32)\n",
    "    X_data = np.hstack([ts[p-lag_idx : n_tp-lag_idx, :] for lag_idx in range(1, p + 1)]).astype(np.float32)\n",
    "    scaler_X, scaler_Y = StandardScaler(), StandardScaler()\n",
    "    try:\n",
    "        X_scaled = scaler_X.fit_transform(X_data)\n",
    "        Y_scaled = scaler_Y.fit_transform(Y_data)\n",
    "    except ValueError as e: logger.error(f\"LassoVAR (S {sid}): Scaling failed - {e}. Skipping.\"); return None\n",
    "    try:\n",
    "        model = MultiTaskLassoCV(cv=5, n_jobs=-1, max_iter=LASSO_MAX_ITER, tol=LASSO_TOL, random_state=42, selection='random', verbose=False)\n",
    "        model.fit(X_scaled, Y_scaled)\n",
    "    except Exception as e: logger.error(f\"LassoVAR (S {sid}): Model fitting failed – {e}\", exc_info=False); return None\n",
    "    var_coeffs_scaled = model.coef_\n",
    "    influence_matrix = np.zeros((n_rois, n_rois), dtype=np.float32)\n",
    "    for target_roi_idx in range(n_rois):\n",
    "        for source_roi_idx in range(n_rois):\n",
    "            coeffs_for_pair = [var_coeffs_scaled[target_roi_idx, source_roi_idx + (lag_k * n_rois)] for lag_k in range(p)]\n",
    "            influence_matrix[target_roi_idx, source_roi_idx] = np.sum(np.abs(coeffs_for_pair))\n",
    "    np.fill_diagonal(influence_matrix, 0.0)\n",
    "    return influence_matrix\n",
    "\n",
    "# --- 4. Function to Calculate All Connectivity Modalities ---\n",
    "def calculate_all_connectivity_modalities_for_subject(\n",
    "    subject_id: str, subject_ts_data: np.ndarray,\n",
    "    n_neighbors_mi_param: int,\n",
    "    dfc_win_points_param: int, dfc_step_param: int,\n",
    "    lasso_lag_param: int\n",
    ") -> Dict[str, Any]:\n",
    "    matrices: Dict[str, Optional[np.ndarray]] = {name: None for name in CONNECTIVITY_CHANNEL_NAMES}\n",
    "    errors_in_calculation: Dict[str, str] = {}\n",
    "    timings: Dict[str, float] = {}\n",
    "\n",
    "    for channel_name in CONNECTIVITY_CHANNEL_NAMES:\n",
    "        logger.info(f\"Calculating {channel_name} for S {subject_id}...\")\n",
    "        start_time_channel = time.time()\n",
    "        matrix_result: Optional[np.ndarray] = None\n",
    "        error_msg: Optional[str] = None\n",
    "\n",
    "        try:\n",
    "            if channel_name == PEARSON_OMST_CHANNEL_NAME:\n",
    "                matrix_result = calculate_pearson_omst(subject_ts_data, subject_id)\n",
    "            elif channel_name == \"MI_KNN\":\n",
    "                matrix_result = calculate_mi_knn_connectivity(subject_ts_data, n_neighbors_mi_param, subject_id)\n",
    "            elif channel_name == \"dFC_AbsDiffMean\":\n",
    "                matrix_result = calculate_custom_dfc_abs_diff_mean(subject_ts_data, dfc_win_points_param, dfc_step_param, subject_id)\n",
    "            elif channel_name.startswith(\"Lasso_VAR\"):\n",
    "                matrix_result = calculate_lasso_var_connectivity_matrix(subject_ts_data, lasso_lag_param, subject_id)\n",
    "            else:\n",
    "                error_msg = f\"Unknown connectivity channel: {channel_name}\"\n",
    "                logger.error(error_msg)\n",
    "\n",
    "            if matrix_result is None and error_msg is None: # If function returned None without raising an exception\n",
    "                error_msg = f\"{channel_name} calculation returned None.\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            logger.error(f\"Error calculating {channel_name} for S {subject_id}: {e}\", exc_info=True if not isinstance(e, (ValueError, TypeError)) else False)\n",
    "        \n",
    "        matrices[channel_name] = matrix_result\n",
    "        if error_msg:\n",
    "            errors_in_calculation[channel_name] = error_msg\n",
    "        \n",
    "        timings[f\"{channel_name}_time_sec\"] = time.time() - start_time_channel\n",
    "        if matrix_result is not None:\n",
    "            logger.info(f\"{channel_name} for S {subject_id} took {timings[f'{channel_name}_time_sec']:.2f}s.\")\n",
    "        else:\n",
    "            logger.warning(f\"{channel_name} for S {subject_id} failed or returned None. Took {timings[f'{channel_name}_time_sec']:.2f}s. Error: {errors_in_calculation.get(channel_name, 'N/A')}\")\n",
    "            \n",
    "    num_modalities_expected = len(CONNECTIVITY_CHANNEL_NAMES)\n",
    "    num_successful = sum(1 for mat_name in CONNECTIVITY_CHANNEL_NAMES if matrices.get(mat_name) is not None)\n",
    "    if num_successful < num_modalities_expected:\n",
    "        logger.warning(f\"Connectivity for S {subject_id}: {num_successful}/{num_modalities_expected} modalities computed. Errors: {errors_in_calculation}\")\n",
    "    else:\n",
    "        logger.info(f\"Connectivity for S {subject_id}: All {num_successful}/{num_modalities_expected} modalities computed successfully.\")\n",
    "\n",
    "    return {\"matrices\": matrices, \"errors_conn_calc\": errors_in_calculation, \"timings_conn_calc\": timings}\n",
    "\n",
    "\n",
    "# --- 5. Per-Subject Processing Pipeline (for Multiprocessing) ---\n",
    "def process_single_subject_pipeline(subject_row_tuple: Tuple[int, pd.Series]) -> Dict[str, Any]:\n",
    "    idx, subject_row = subject_row_tuple\n",
    "    subject_id = str(subject_row['SubjectID']).strip()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_initial_mb = process.memory_info().rss / (1024**2)\n",
    "    result: Dict[str, Any] = {\n",
    "        \"id\": subject_id, \"status_preprocessing\": \"PENDING\", \"detail_preprocessing\": \"\",\n",
    "        \"status_connectivity_calc\": \"NOT_ATTEMPTED\", \"errors_connectivity_calc\": {},\n",
    "        \"timings_connectivity_calc_sec\": {}, \"path_saved_tensor\": None,\n",
    "        \"status_overall\": \"PENDING\",\n",
    "        \"ram_usage_mb_initial\": ram_initial_mb, \"ram_usage_mb_final\": -1.0\n",
    "    }\n",
    "    series_data: Optional[np.ndarray] = None\n",
    "    try:\n",
    "        series_data, detail_msg_preproc, success_preproc = load_and_preprocess_single_subject_series(\n",
    "            subject_id, N_ROIS_EXPECTED, TARGET_LEN_TS,\n",
    "            ROI_SIGNALS_DIR_PATH_AAL3, ROI_FILENAME_TEMPLATE, POSSIBLE_ROI_KEYS,\n",
    "            LASSO_VAR_MAX_LAG, TR_SECONDS, LOW_CUT_HZ, HIGH_CUT_HZ, FILTER_ORDER,\n",
    "            APPLY_HRF_DECONVOLUTION, HRF_MODEL\n",
    "        )\n",
    "        result[\"status_preprocessing\"] = \"SUCCESS\" if success_preproc else \"FAILED\"\n",
    "        result[\"detail_preprocessing\"] = detail_msg_preproc\n",
    "        if not success_preproc or series_data is None:\n",
    "            result[\"status_overall\"] = \"PREPROCESSING_FAILED\"; return result\n",
    "\n",
    "        connectivity_results = calculate_all_connectivity_modalities_for_subject(\n",
    "            subject_id, series_data, N_NEIGHBORS_MI,\n",
    "            DFC_WIN_POINTS, DFC_STEP, LASSO_VAR_MAX_LAG\n",
    "        )\n",
    "        del series_data; series_data = None; gc.collect()\n",
    "        calculated_matrices_dict = connectivity_results[\"matrices\"]\n",
    "        result[\"errors_connectivity_calc\"] = connectivity_results[\"errors_conn_calc\"]\n",
    "        result[\"timings_connectivity_calc_sec\"] = connectivity_results.get(\"timings_conn_calc\", {})\n",
    "\n",
    "        all_modalities_valid = True\n",
    "        final_matrices_to_stack_list = []\n",
    "        expected_matrix_shape = (N_ROIS_EXPECTED, N_ROIS_EXPECTED)\n",
    "        for channel_name in CONNECTIVITY_CHANNEL_NAMES: # Use the global, dynamically built list\n",
    "            matrix = calculated_matrices_dict.get(channel_name)\n",
    "            if matrix is None:\n",
    "                all_modalities_valid = False; err_msg = f\"Modality '{channel_name}' None.\"\n",
    "                logger.error(f\"S {subject_id}: {err_msg}\")\n",
    "                result[\"errors_connectivity_calc\"][channel_name] = result[\"errors_connectivity_calc\"].get(channel_name, \"\") + \" | \" + err_msg\n",
    "                break\n",
    "            elif matrix.shape != expected_matrix_shape:\n",
    "                all_modalities_valid = False; err_msg = f\"Modality '{channel_name}' shape {matrix.shape} != {expected_matrix_shape}.\"\n",
    "                logger.error(f\"S {subject_id}: {err_msg}\")\n",
    "                result[\"errors_connectivity_calc\"][channel_name] = result[\"errors_connectivity_calc\"].get(channel_name, \"\") + \" | \" + err_msg\n",
    "                break\n",
    "            else:\n",
    "                final_matrices_to_stack_list.append(matrix)\n",
    "        \n",
    "        if all_modalities_valid and len(final_matrices_to_stack_list) == N_CHANNELS: # N_CHANNELS is global\n",
    "            result[\"status_connectivity_calc\"] = \"SUCCESS_ALL_MODALITIES_VALID\"\n",
    "            try:\n",
    "                subject_tensor = np.stack(final_matrices_to_stack_list, axis=0).astype(np.float32)\n",
    "                del final_matrices_to_stack_list, calculated_matrices_dict; gc.collect()\n",
    "                output_dir_individual_tensors = BASE_PATH_AAL3 / OUTPUT_CONNECTIVITY_DIR_NAME / \"individual_subject_tensors\"\n",
    "                output_dir_individual_tensors.mkdir(parents=True, exist_ok=True)\n",
    "                output_path = output_dir_individual_tensors / f\"tensor_{N_CHANNELS}ch_{subject_id}.npz\"\n",
    "                np.savez_compressed(output_path, tensor_data=subject_tensor, subject_id=subject_id, channel_names=np.array(CONNECTIVITY_CHANNEL_NAMES, dtype=str))\n",
    "                result[\"path_saved_tensor\"] = str(output_path)\n",
    "                result[\"status_overall\"] = \"SUCCESS_ALL_PROCESSED_AND_SAVED\"\n",
    "                del subject_tensor; gc.collect()\n",
    "            except Exception as e_save:\n",
    "                logger.error(f\"Error saving tensor S {subject_id}: {e_save}\", exc_info=True)\n",
    "                result[\"errors_connectivity_calc\"][\"save_error\"] = str(e_save)\n",
    "                result[\"status_overall\"] = \"FAILURE_DURING_TENSOR_SAVING\"\n",
    "                result[\"status_connectivity_calc\"] = \"FAILURE_DURING_SAVING\"\n",
    "        else:\n",
    "            result[\"status_overall\"] = \"FAILURE_IN_CONNECTIVITY_CALC_OR_VALIDATION\"\n",
    "            result[\"status_connectivity_calc\"] = \"FAILURE_MISSING_OR_INVALID_MODALITIES\"\n",
    "            if not all_modalities_valid: logger.error(f\"S {subject_id}: Not all modalities valid. Tensor not saved. Errors: {result['errors_connectivity_calc']}\")\n",
    "    finally:\n",
    "        if series_data is not None: del series_data; gc.collect()\n",
    "        result[\"ram_usage_mb_final\"] = process.memory_info().rss / (1024**2)\n",
    "    return result\n",
    "\n",
    "# --- 6. Main Script Execution Flow ---\n",
    "def main():\n",
    "    script_start_time = time.time()\n",
    "    main_process_info = psutil.Process(os.getpid())\n",
    "    logger.info(f\"Main process RAM at start: {main_process_info.memory_info().rss / (1024**2):.2f} MB\")\n",
    "    logger.info(f\"--- Starting fMRI Connectivity Pipeline (Luppi et al. 2024 informed) ---\")\n",
    "    logger.info(f\"--- Output Directory: {OUTPUT_CONNECTIVITY_DIR_NAME} ---\")\n",
    "    logger.info(f\"--- N_ROIS_EXPECTED (final for connectivity): {N_ROIS_EXPECTED} ---\")\n",
    "    logger.info(f\"--- TARGET_LEN_TS (homogenized time series length): {TARGET_LEN_TS} ---\")\n",
    "\n",
    "    if not BASE_PATH_AAL3.exists() or not ROI_SIGNALS_DIR_PATH_AAL3.exists():\n",
    "        logger.critical(\"CRITICAL: Base AAL3 path or ROI signals directory not found. Aborting.\")\n",
    "        return\n",
    "\n",
    "    subject_metadata_df = load_metadata(SUBJECT_METADATA_CSV_PATH, QC_REPORT_CSV_PATH)\n",
    "    if subject_metadata_df is None or subject_metadata_df.empty:\n",
    "        logger.critical(\"Metadata loading failed or no subjects passed QC. Aborting.\")\n",
    "        return\n",
    "\n",
    "    output_main_directory = BASE_PATH_AAL3 / OUTPUT_CONNECTIVITY_DIR_NAME\n",
    "    output_individual_tensors_dir = output_main_directory / \"individual_subject_tensors\"\n",
    "    try:\n",
    "        output_main_directory.mkdir(parents=True, exist_ok=True)\n",
    "        output_individual_tensors_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Main output directory: {output_main_directory}\")\n",
    "    except OSError as e:\n",
    "        logger.critical(f\"Could not create output directories: {e}. Aborting.\"); return\n",
    "\n",
    "    total_cpu_cores = multiprocessing.cpu_count()\n",
    "    MAX_WORKERS = max(1, total_cpu_cores // 2) \n",
    "    logger.info(f\"Total CPU cores: {total_cpu_cores}. Using MAX_WORKERS = {MAX_WORKERS}.\")\n",
    "    available_ram_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    logger.warning(f\"Available system RAM: {available_ram_gb:.2f} GB. Monitor usage.\")\n",
    "\n",
    "    subject_rows_to_process = list(subject_metadata_df.iterrows())\n",
    "    num_subjects_to_process = len(subject_rows_to_process)\n",
    "    if num_subjects_to_process == 0: logger.critical(\"No subjects to process. Aborting.\"); return\n",
    "    logger.info(f\"Starting parallel processing for {num_subjects_to_process} subjects.\")\n",
    "    \n",
    "    all_subject_results_list = []\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_subject_id_map = {\n",
    "            executor.submit(process_single_subject_pipeline, subject_tuple): str(subject_tuple[1]['SubjectID']).strip()\n",
    "            for subject_tuple in subject_rows_to_process\n",
    "        }\n",
    "        for future in tqdm(as_completed(future_to_subject_id_map), total=num_subjects_to_process, desc=\"Processing Subjects\"):\n",
    "            subject_id_for_log = future_to_subject_id_map[future]\n",
    "            try:\n",
    "                subject_result = future.result()\n",
    "                all_subject_results_list.append(subject_result)\n",
    "            except Exception as exc:\n",
    "                logger.critical(f\"CRITICAL WORKER EXCEPTION S {subject_id_for_log}: {exc}\", exc_info=True)\n",
    "                all_subject_results_list.append({\n",
    "                    \"id\": subject_id_for_log, \"status_overall\": \"CRITICAL_WORKER_EXCEPTION\",\n",
    "                    \"detail_preprocessing\": str(exc), \"errors_connectivity_calc\": {\"worker_exception\": str(exc)}\n",
    "                })\n",
    "\n",
    "    processing_log_df = pd.DataFrame(all_subject_results_list)\n",
    "    log_file_path = output_main_directory / f\"pipeline_log_{output_main_directory.name}.csv\"\n",
    "    try: processing_log_df.to_csv(log_file_path, index=False); logger.info(f\"Processing log saved: {log_file_path}\")\n",
    "    except Exception as e_log_save: logger.error(f\"Failed to save processing log: {e_log_save}\")\n",
    "\n",
    "    successful_subject_entries_list = [\n",
    "        res for res in all_subject_results_list\n",
    "        if res.get(\"status_overall\") == \"SUCCESS_ALL_PROCESSED_AND_SAVED\" and \\\n",
    "           res.get(\"path_saved_tensor\") and Path(res[\"path_saved_tensor\"]).exists()\n",
    "    ]\n",
    "    num_successful_subjects = len(successful_subject_entries_list)\n",
    "    logger.info(f\"--- Processing Summary ---\")\n",
    "    logger.info(f\"Total subjects attempted: {num_subjects_to_process}\")\n",
    "    logger.info(f\"Successfully processed and individual tensors saved: {num_successful_subjects}\")\n",
    "    if num_successful_subjects < num_subjects_to_process:\n",
    "        logger.warning(f\"{num_subjects_to_process - num_successful_subjects} subjects failed. Check log.\")\n",
    "\n",
    "    if num_successful_subjects > 0:\n",
    "        logger.info(f\"Assembling global tensor for {num_successful_subjects} subjects.\")\n",
    "        global_conn_tensor_list = []\n",
    "        final_subject_ids_in_tensor = []\n",
    "        try:\n",
    "            for s_entry in tqdm(successful_subject_entries_list, desc=\"Assembling Global Tensor\"):\n",
    "                s_id, tensor_path_str = s_entry[\"id\"], s_entry[\"path_saved_tensor\"]\n",
    "                try:\n",
    "                    with np.load(tensor_path_str) as loaded_npz:\n",
    "                        s_tensor_data = loaded_npz['tensor_data']\n",
    "                        if s_tensor_data.shape == (N_CHANNELS, N_ROIS_EXPECTED, N_ROIS_EXPECTED):\n",
    "                            global_conn_tensor_list.append(s_tensor_data)\n",
    "                            final_subject_ids_in_tensor.append(s_id)\n",
    "                        else: logger.error(f\"Tensor S {s_id} shape mismatch. Skipping.\")\n",
    "                    del s_tensor_data; gc.collect()\n",
    "                except Exception as e_load_ind_tensor: logger.error(f\"Error loading S {s_id} tensor: {e_load_ind_tensor}. Skipping.\")\n",
    "            \n",
    "            if global_conn_tensor_list:\n",
    "                global_conn_tensor = np.stack(global_conn_tensor_list, axis=0).astype(np.float32)\n",
    "                del global_conn_tensor_list; gc.collect()\n",
    "                global_tensor_fname = f\"GLOBAL_TENSOR_AAL3_{N_ROIS_EXPECTED}_{len(final_subject_ids_in_tensor)}subs_{N_CHANNELS}ch_{LASSO_VAR_MAX_LAG}lag{deconv_str}.npz\"\n",
    "                global_tensor_path = output_main_directory / global_tensor_fname\n",
    "                np.savez_compressed(global_tensor_path, global_tensor_data=global_conn_tensor, subject_ids=np.array(final_subject_ids_in_tensor, dtype=str), channel_names=np.array(CONNECTIVITY_CHANNEL_NAMES, dtype=str))\n",
    "                logger.info(f\"Global tensor saved: {global_tensor_path} (Shape: {global_conn_tensor.shape})\")\n",
    "                del global_conn_tensor; gc.collect()\n",
    "            else: logger.warning(\"No valid individual tensors for global assembly.\")\n",
    "        except MemoryError: logger.critical(\"MEMORY ERROR during global tensor assembly.\")\n",
    "        except Exception as e_global: logger.critical(f\"Error during global tensor assembly: {e_global}\", exc_info=True)\n",
    "\n",
    "    total_time_min = (time.time() - script_start_time) / 60\n",
    "    logger.info(f\"--- fMRI Connectivity Pipeline Finished ---\")\n",
    "    logger.info(f\"Total execution time: {total_time_min:.2f} minutes.\")\n",
    "    logger.info(f\"Final main process RAM: {main_process_info.memory_info().rss / (1024**2):.2f} MB\")\n",
    "    logger.info(f\"Outputs in: {output_main_directory}\")\n",
    "    logger.info(\"THESIS REMINDER: Document parameters, QC, subject selection, AAL3 ROI definitions.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.freeze_support() \n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
