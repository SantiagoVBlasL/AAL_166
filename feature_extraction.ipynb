{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6fdd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: networkx 2.6.3\n",
      "Uninstalling networkx-2.6.3:\n",
      "  Successfully uninstalled networkx-2.6.3\n",
      "Collecting networkx==2.6.3\n",
      "  Using cached networkx-2.6.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Using cached networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "Installing collected packages: networkx\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-image 0.20.0 requires networkx>=2.8, but you have networkx 2.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed networkx-2.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall networkx -y\n",
    "!pip install networkx==2.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0e7c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dyconnmap in /home/diego/anaconda3/lib/python3.11/site-packages (1.0.4)\n",
      "Requirement already satisfied: numpy in /home/diego/anaconda3/lib/python3.11/site-packages (from dyconnmap) (1.26.3)\n",
      "Requirement already satisfied: scipy in /home/diego/anaconda3/lib/python3.11/site-packages (from dyconnmap) (1.11.4)\n",
      "Requirement already satisfied: networkx in /home/diego/anaconda3/lib/python3.11/site-packages (from dyconnmap) (2.6.3)\n",
      "Requirement already satisfied: matplotlib in /home/diego/anaconda3/lib/python3.11/site-packages (from dyconnmap) (3.8.0)\n",
      "Requirement already satisfied: statsmodels in /home/diego/anaconda3/lib/python3.11/site-packages (from dyconnmap) (0.14.0)\n",
      "Requirement already satisfied: scikit-learn in /home/diego/anaconda3/lib/python3.11/site-packages (from dyconnmap) (1.2.2)\n",
      "Requirement already satisfied: bctpy in /home/diego/anaconda3/lib/python3.11/site-packages (from dyconnmap) (0.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/diego/anaconda3/lib/python3.11/site-packages (from matplotlib->dyconnmap) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/diego/anaconda3/lib/python3.11/site-packages (from matplotlib->dyconnmap) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/diego/anaconda3/lib/python3.11/site-packages (from matplotlib->dyconnmap) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/diego/anaconda3/lib/python3.11/site-packages (from matplotlib->dyconnmap) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/diego/anaconda3/lib/python3.11/site-packages (from matplotlib->dyconnmap) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/diego/anaconda3/lib/python3.11/site-packages (from matplotlib->dyconnmap) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/diego/anaconda3/lib/python3.11/site-packages (from matplotlib->dyconnmap) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/diego/anaconda3/lib/python3.11/site-packages (from matplotlib->dyconnmap) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/diego/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->dyconnmap) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/diego/anaconda3/lib/python3.11/site-packages (from scikit-learn->dyconnmap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/diego/anaconda3/lib/python3.11/site-packages (from scikit-learn->dyconnmap) (2.2.0)\n",
      "Requirement already satisfied: pandas>=1.0 in /home/diego/anaconda3/lib/python3.11/site-packages (from statsmodels->dyconnmap) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /home/diego/anaconda3/lib/python3.11/site-packages (from statsmodels->dyconnmap) (0.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/diego/anaconda3/lib/python3.11/site-packages (from pandas>=1.0->statsmodels->dyconnmap) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/diego/anaconda3/lib/python3.11/site-packages (from pandas>=1.0->statsmodels->dyconnmap) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install dyconnmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c259576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:29:01,669 - INFO - 335981393.py:50 - Successfully imported 'threshold_omst_global_cost_efficiency' from 'dyconnmap.graphs.threshold' and aliased as 'orthogonal_minimum_spanning_tree'.\n",
      "2025-06-01 14:29:01,671 - INFO - 335981393.py:127 - --- Initializing AAL3 ROI Processing Information ---\n",
      "2025-06-01 14:29:01,675 - INFO - 335981393.py:164 - AAL3 ROI processing info initialized:\n",
      "2025-06-01 14:29:01,675 - INFO - 335981393.py:165 -   Indices of 4 AAL3 systemically missing ROIs (0-based, from 170): [34, 35, 80, 81]\n",
      "2025-06-01 14:29:01,675 - INFO - 335981393.py:166 -   Number of ROIs in AAL3 meta after excluding systemically missing: 166 (Expected: 166)\n",
      "2025-06-01 14:29:01,676 - INFO - 335981393.py:167 -   Indices of small ROIs to drop (from the 166 set, 0-based): [108, 116, 117, 118, 119, 120, 121, 126, 127, 128, 129, 132, 133, 134, 135, 136, 137, 138, 139, 142, 143, 144, 145, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165]\n",
      "2025-06-01 14:29:01,676 - INFO - 335981393.py:168 -   Number of small ROIs to drop: 35\n",
      "2025-06-01 14:29:01,677 - INFO - 335981393.py:169 -   FINAL_N_ROIS_EXPECTED for connectivity analysis: 131 (This should be 131 if matching QC script)\n",
      "2025-06-01 14:29:01,677 - INFO - 335981393.py:196 - Final N_ROIS_EXPECTED after initialization: 131\n",
      "2025-06-01 14:29:01,678 - INFO - 335981393.py:197 - Final OUTPUT_CONNECTIVITY_DIR_NAME: AAL3_131ROIs_fmri_tensor_OMST_GCE_1lag_NeuroEnhanced_v6.5.3_ElasticNetRelaxed\n",
      "2025-06-01 14:29:01,678 - INFO - 335981393.py:198 - Connectivity channels to be computed: ['Pearson_OMST_GCE_Weighted', 'MI_KNN_Symmetric', 'dFC_AbsDiffMean', 'ElasticNet_VAR_Influence']\n",
      "2025-06-01 14:29:01,679 - INFO - 335981393.py:199 - Total number of channels (for VAE): 4\n",
      "2025-06-01 14:29:01,687 - INFO - 335981393.py:1037 - RUNTIME NetworkX version being used: 2.6.3\n",
      "2025-06-01 14:29:01,688 - INFO - 335981393.py:1046 - Main process RAM at start: 283.11 MB\n",
      "2025-06-01 14:29:01,688 - INFO - 335981393.py:1047 - --- Starting fMRI Connectivity Pipeline (Version for Doctoral Thesis with Dyconnmap v6.5.3_ElasticNetRelaxed) ---\n",
      "2025-06-01 14:29:01,689 - INFO - 335981393.py:1049 - --- Final Expected ROIs for Connectivity Matrices: 131 (should be 131) ---\n",
      "2025-06-01 14:29:01,689 - INFO - 335981393.py:1050 - --- Target Homogenized Time Series Length: 140 ---\n",
      "2025-06-01 14:29:01,690 - INFO - 335981393.py:1051 - --- Output Directory Name: AAL3_131ROIs_fmri_tensor_OMST_GCE_1lag_NeuroEnhanced_v6.5.3_ElasticNetRelaxed ---\n",
      "2025-06-01 14:29:01,690 - INFO - 335981393.py:1052 - --- Selected Connectivity Channels for VAE: ['Pearson_OMST_GCE_Weighted', 'MI_KNN_Symmetric', 'dFC_AbsDiffMean', 'ElasticNet_VAR_Influence'] (4 channels) ---\n",
      "2025-06-01 14:29:01,690 - INFO - 335981393.py:208 - --- Starting Subject Metadata Loading and QC Integration ---\n",
      "2025-06-01 14:29:01,693 - INFO - 335981393.py:219 - Loaded main metadata from /home/diego/Escritorio/AAL3/SubjectsData_Schaefer2018_400ROIs.csv. Shape: (434, 24)\n",
      "2025-06-01 14:29:01,696 - INFO - 335981393.py:227 - Loaded QC report from /home/diego/Escritorio/AAL3/qc_outputs_doctoral_v3.2_aal3_shrinkage_flexible_thresh_fix/report_qc_final_with_discard_flags_v3.2.csv. Shape: (434, 34)\n",
      "2025-06-01 14:29:01,696 - INFO - 335981393.py:230 - Found 'Subject' column in QC report, renaming to 'SubjectID'.\n",
      "2025-06-01 14:29:01,702 - INFO - 335981393.py:260 - Total subjects after merge: 434\n",
      "2025-06-01 14:29:01,703 - INFO - 335981393.py:261 - Subjects discarded based on QC ('ToDiscard_Overall' == True): 3\n",
      "2025-06-01 14:29:01,703 - INFO - 335981393.py:262 - Subjects passing QC and to be processed: 431\n",
      "2025-06-01 14:29:01,703 - INFO - 335981393.py:270 - Timepoints for subjects passing QC (from QC report): Min=140, Max=200.\n",
      "2025-06-01 14:29:01,704 - INFO - 335981393.py:271 - These will be homogenized to TARGET_LEN_TS = 140 for connectivity calculation.\n",
      "2025-06-01 14:29:01,705 - INFO - 335981393.py:1071 - Main output directory created/exists: /home/diego/Escritorio/AAL3/AAL3_131ROIs_fmri_tensor_OMST_GCE_1lag_NeuroEnhanced_v6.5.3_ElasticNetRelaxed\n",
      "2025-06-01 14:29:01,706 - INFO - 335981393.py:1077 - Total CPU cores available: 12. Using MAX_WORKERS = 6.\n",
      "2025-06-01 14:29:01,706 - WARNING - 335981393.py:1079 - Available system RAM at start of parallel processing: 18.98 GB. Monitor usage closely.\n",
      "2025-06-01 14:29:01,720 - INFO - 335981393.py:1086 - Starting parallel processing for 431 subjects.\n",
      "Processing Subjects:   0%|          | 0/431 [00:00<?, ?it/s]2025-06-01 14:29:01,779 - INFO - 335981393.py:354 - S 002_S_0413: Removed 4 known missing AAL3 ROIs. Shape (140, 170) -> (140, 166)\n",
      "2025-06-01 14:29:01,781 - INFO - 335981393.py:370 - S 002_S_0413: Removed 35 small ROIs. Shape (140, 166) -> (140, 131)\n",
      "2025-06-01 14:29:01,780 - INFO - 335981393.py:354 - S 002_S_0685: Removed 4 known missing AAL3 ROIs. Shape (140, 170) -> (140, 166)\n",
      "2025-06-01 14:29:01,780 - INFO - 335981393.py:354 - S 002_S_0295: Removed 4 known missing AAL3 ROIs. Shape (140, 170) -> (140, 166)\n",
      "2025-06-01 14:29:01,782 - INFO - 335981393.py:381 - S 002_S_0413: Final ROI count 131 matches FINAL_N_ROIS_EXPECTED 131.\n",
      "2025-06-01 14:29:01,780 - INFO - 335981393.py:354 - S 002_S_0729: Removed 4 known missing AAL3 ROIs. Shape (140, 170) -> (140, 166)\n",
      "2025-06-01 14:29:01,782 - INFO - 335981393.py:370 - S 002_S_0685: Removed 35 small ROIs. Shape (140, 166) -> (140, 131)\n",
      "2025-06-01 14:29:01,781 - INFO - 335981393.py:354 - S 002_S_1155: Removed 4 known missing AAL3 ROIs. Shape (197, 170) -> (197, 166)\n",
      "2025-06-01 14:29:01,783 - INFO - 335981393.py:381 - S 002_S_0685: Final ROI count 131 matches FINAL_N_ROIS_EXPECTED 131.\n",
      "2025-06-01 14:29:01,783 - INFO - 335981393.py:370 - S 002_S_0295: Removed 35 small ROIs. Shape (140, 166) -> (140, 131)\n",
      "2025-06-01 14:29:01,783 - INFO - 335981393.py:370 - S 002_S_0729: Removed 35 small ROIs. Shape (140, 166) -> (140, 131)\n",
      "2025-06-01 14:29:01,783 - INFO - 335981393.py:354 - S 002_S_2010: Removed 4 known missing AAL3 ROIs. Shape (140, 170) -> (140, 166)\n",
      "2025-06-01 14:29:01,784 - INFO - 335981393.py:381 - S 002_S_0729: Final ROI count 131 matches FINAL_N_ROIS_EXPECTED 131.\n",
      "2025-06-01 14:29:01,785 - INFO - 335981393.py:370 - S 002_S_2010: Removed 35 small ROIs. Shape (140, 166) -> (140, 131)\n",
      "2025-06-01 14:29:01,785 - INFO - 335981393.py:381 - S 002_S_2010: Final ROI count 131 matches FINAL_N_ROIS_EXPECTED 131.\n",
      "2025-06-01 14:29:01,784 - INFO - 335981393.py:370 - S 002_S_1155: Removed 35 small ROIs. Shape (197, 166) -> (197, 131)\n",
      "2025-06-01 14:29:01,791 - INFO - 335981393.py:381 - S 002_S_1155: Final ROI count 131 matches FINAL_N_ROIS_EXPECTED 131.\n",
      "2025-06-01 14:29:01,795 - INFO - 335981393.py:381 - S 002_S_0295: Final ROI count 131 matches FINAL_N_ROIS_EXPECTED 131.\n",
      "2025-06-01 14:29:01,935 - INFO - 335981393.py:474 - S 002_S_2010: Preprocessing. Input TPs: 140, ROIs: 131 (should be 131), TR: 3.0s. Target TPs for output: 140.\n",
      "2025-06-01 14:29:01,940 - INFO - 335981393.py:474 - S 002_S_0685: Preprocessing. Input TPs: 140, ROIs: 131 (should be 131), TR: 3.0s. Target TPs for output: 140.\n",
      "2025-06-01 14:29:01,940 - INFO - 335981393.py:474 - S 002_S_0729: Preprocessing. Input TPs: 140, ROIs: 131 (should be 131), TR: 3.0s. Target TPs for output: 140.\n",
      "2025-06-01 14:29:01,940 - INFO - 335981393.py:474 - S 002_S_0413: Preprocessing. Input TPs: 140, ROIs: 131 (should be 131), TR: 3.0s. Target TPs for output: 140.\n",
      "2025-06-01 14:29:01,947 - INFO - 335981393.py:474 - S 002_S_1155: Preprocessing. Input TPs: 197, ROIs: 131 (should be 131), TR: 3.0s. Target TPs for output: 140.\n",
      "2025-06-01 14:29:01,957 - INFO - 335981393.py:474 - S 002_S_0295: Preprocessing. Input TPs: 140, ROIs: 131 (should be 131), TR: 3.0s. Target TPs for output: 140.\n",
      "2025-06-01 14:29:01,969 - INFO - 335981393.py:519 - S 002_S_1155: Homogenizing time series length from 197 to 140.\n",
      "2025-06-01 14:29:02,062 - INFO - 335981393.py:603 - S 002_S_0413: Successfully loaded and preprocessed. Original TPs: 140, Final Shape for conn: (140, 131)\n",
      "2025-06-01 14:29:02,063 - INFO - 335981393.py:862 - Calculating Pearson_OMST_GCE_Weighted for S 002_S_0413 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:02,078 - INFO - 335981393.py:603 - S 002_S_0685: Successfully loaded and preprocessed. Original TPs: 140, Final Shape for conn: (140, 131)\n",
      "2025-06-01 14:29:02,078 - INFO - 335981393.py:603 - S 002_S_0729: Successfully loaded and preprocessed. Original TPs: 140, Final Shape for conn: (140, 131)\n",
      "2025-06-01 14:29:02,078 - INFO - 335981393.py:603 - S 002_S_2010: Successfully loaded and preprocessed. Original TPs: 140, Final Shape for conn: (140, 131)\n",
      "2025-06-01 14:29:02,079 - INFO - 335981393.py:862 - Calculating Pearson_OMST_GCE_Weighted for S 002_S_2010 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:02,079 - INFO - 335981393.py:862 - Calculating Pearson_OMST_GCE_Weighted for S 002_S_0685 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:02,079 - INFO - 335981393.py:862 - Calculating Pearson_OMST_GCE_Weighted for S 002_S_0729 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:02,084 - INFO - 335981393.py:603 - S 002_S_0295: Successfully loaded and preprocessed. Original TPs: 140, Final Shape for conn: (140, 131)\n",
      "2025-06-01 14:29:02,085 - INFO - 335981393.py:862 - Calculating Pearson_OMST_GCE_Weighted for S 002_S_0295 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:02,085 - INFO - 335981393.py:603 - S 002_S_1155: Successfully loaded and preprocessed. Original TPs: 197, Final Shape for conn: (140, 131)\n",
      "2025-06-01 14:29:02,086 - INFO - 335981393.py:862 - Calculating Pearson_OMST_GCE_Weighted for S 002_S_1155 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:02,101 - INFO - 335981393.py:668 - S 002_S_0295: Calling dyconnmap.threshold_omst_global_cost_efficiency with weights shape (131, 131)\n",
      "2025-06-01 14:29:02,093 - INFO - 335981393.py:668 - S 002_S_2010: Calling dyconnmap.threshold_omst_global_cost_efficiency with weights shape (131, 131)\n",
      "2025-06-01 14:29:02,123 - INFO - 335981393.py:668 - S 002_S_0685: Calling dyconnmap.threshold_omst_global_cost_efficiency with weights shape (131, 131)\n",
      "2025-06-01 14:29:02,151 - INFO - 335981393.py:668 - S 002_S_0413: Calling dyconnmap.threshold_omst_global_cost_efficiency with weights shape (131, 131)\n",
      "2025-06-01 14:29:02,182 - INFO - 335981393.py:668 - S 002_S_0729: Calling dyconnmap.threshold_omst_global_cost_efficiency with weights shape (131, 131)\n",
      "2025-06-01 14:29:02,183 - INFO - 335981393.py:668 - S 002_S_1155: Calling dyconnmap.threshold_omst_global_cost_efficiency with weights shape (131, 131)\n",
      "2025-06-01 14:29:51,253 - INFO - 335981393.py:685 - Pearson_OMST_GCE (S 002_S_1155): Successfully calculated using dyconnmap. Matrix density: 0.0758\n",
      "2025-06-01 14:29:51,256 - INFO - 335981393.py:900 - Pearson_OMST_GCE_Weighted for S 002_S_1155 calculated. Shape: (131, 131). Took 49.17s.\n",
      "2025-06-01 14:29:51,256 - INFO - 335981393.py:862 - Calculating MI_KNN_Symmetric for S 002_S_1155 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:52,715 - INFO - 335981393.py:685 - Pearson_OMST_GCE (S 002_S_0295): Successfully calculated using dyconnmap. Matrix density: 0.0758\n",
      "2025-06-01 14:29:52,718 - INFO - 335981393.py:900 - Pearson_OMST_GCE_Weighted for S 002_S_0295 calculated. Shape: (131, 131). Took 50.63s.\n",
      "2025-06-01 14:29:52,718 - INFO - 335981393.py:862 - Calculating MI_KNN_Symmetric for S 002_S_0295 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:53,836 - INFO - 335981393.py:685 - Pearson_OMST_GCE (S 002_S_0685): Successfully calculated using dyconnmap. Matrix density: 0.0758\n",
      "2025-06-01 14:29:53,838 - INFO - 335981393.py:900 - Pearson_OMST_GCE_Weighted for S 002_S_0685 calculated. Shape: (131, 131). Took 51.76s.\n",
      "2025-06-01 14:29:53,839 - INFO - 335981393.py:862 - Calculating MI_KNN_Symmetric for S 002_S_0685 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:54,801 - INFO - 335981393.py:685 - Pearson_OMST_GCE (S 002_S_0413): Successfully calculated using dyconnmap. Matrix density: 0.0758\n",
      "2025-06-01 14:29:54,804 - INFO - 335981393.py:900 - Pearson_OMST_GCE_Weighted for S 002_S_0413 calculated. Shape: (131, 131). Took 52.74s.\n",
      "2025-06-01 14:29:54,804 - INFO - 335981393.py:862 - Calculating MI_KNN_Symmetric for S 002_S_0413 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:55,924 - INFO - 335981393.py:685 - Pearson_OMST_GCE (S 002_S_0729): Successfully calculated using dyconnmap. Matrix density: 0.0606\n",
      "2025-06-01 14:29:55,926 - INFO - 335981393.py:900 - Pearson_OMST_GCE_Weighted for S 002_S_0729 calculated. Shape: (131, 131). Took 53.85s.\n",
      "2025-06-01 14:29:55,927 - INFO - 335981393.py:862 - Calculating MI_KNN_Symmetric for S 002_S_0729 (TS shape: (140, 131))...\n",
      "2025-06-01 14:29:56,077 - INFO - 335981393.py:685 - Pearson_OMST_GCE (S 002_S_2010): Successfully calculated using dyconnmap. Matrix density: 0.0758\n",
      "2025-06-01 14:29:56,080 - INFO - 335981393.py:900 - Pearson_OMST_GCE_Weighted for S 002_S_2010 calculated. Shape: (131, 131). Took 54.00s.\n",
      "2025-06-01 14:29:56,080 - INFO - 335981393.py:862 - Calculating MI_KNN_Symmetric for S 002_S_2010 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:11,897 - INFO - 335981393.py:728 - MI_KNN_Symmetric (S 002_S_1155): Successfully calculated.\n",
      "2025-06-01 14:30:11,898 - INFO - 335981393.py:900 - MI_KNN_Symmetric for S 002_S_1155 calculated. Shape: (131, 131). Took 20.64s.\n",
      "2025-06-01 14:30:11,899 - INFO - 335981393.py:862 - Calculating dFC_AbsDiffMean for S 002_S_1155 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:11,907 - INFO - 335981393.py:900 - dFC_AbsDiffMean for S 002_S_1155 calculated. Shape: (131, 131). Took 0.01s.\n",
      "2025-06-01 14:30:11,907 - INFO - 335981393.py:862 - Calculating ElasticNet_VAR_Influence for S 002_S_1155 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:13,453 - INFO - 335981393.py:728 - MI_KNN_Symmetric (S 002_S_0295): Successfully calculated.\n",
      "2025-06-01 14:30:13,454 - INFO - 335981393.py:900 - MI_KNN_Symmetric for S 002_S_0295 calculated. Shape: (131, 131). Took 20.74s.\n",
      "2025-06-01 14:30:13,454 - INFO - 335981393.py:862 - Calculating dFC_AbsDiffMean for S 002_S_0295 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:13,465 - INFO - 335981393.py:900 - dFC_AbsDiffMean for S 002_S_0295 calculated. Shape: (131, 131). Took 0.01s.\n",
      "2025-06-01 14:30:13,476 - INFO - 335981393.py:862 - Calculating ElasticNet_VAR_Influence for S 002_S_0295 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:15,053 - INFO - 335981393.py:728 - MI_KNN_Symmetric (S 002_S_0685): Successfully calculated.\n",
      "2025-06-01 14:30:15,053 - INFO - 335981393.py:900 - MI_KNN_Symmetric for S 002_S_0685 calculated. Shape: (131, 131). Took 21.21s.\n",
      "2025-06-01 14:30:15,054 - INFO - 335981393.py:862 - Calculating dFC_AbsDiffMean for S 002_S_0685 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:15,086 - INFO - 335981393.py:900 - dFC_AbsDiffMean for S 002_S_0685 calculated. Shape: (131, 131). Took 0.03s.\n",
      "2025-06-01 14:30:15,104 - INFO - 335981393.py:862 - Calculating ElasticNet_VAR_Influence for S 002_S_0685 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:15,228 - INFO - 335981393.py:728 - MI_KNN_Symmetric (S 002_S_0413): Successfully calculated.\n",
      "2025-06-01 14:30:15,235 - INFO - 335981393.py:900 - MI_KNN_Symmetric for S 002_S_0413 calculated. Shape: (131, 131). Took 20.43s.\n",
      "2025-06-01 14:30:15,239 - INFO - 335981393.py:862 - Calculating dFC_AbsDiffMean for S 002_S_0413 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:15,255 - INFO - 335981393.py:900 - dFC_AbsDiffMean for S 002_S_0413 calculated. Shape: (131, 131). Took 0.01s.\n",
      "2025-06-01 14:30:15,260 - INFO - 335981393.py:862 - Calculating ElasticNet_VAR_Influence for S 002_S_0413 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:16,492 - INFO - 335981393.py:728 - MI_KNN_Symmetric (S 002_S_0729): Successfully calculated.\n",
      "2025-06-01 14:30:16,493 - INFO - 335981393.py:900 - MI_KNN_Symmetric for S 002_S_0729 calculated. Shape: (131, 131). Took 20.57s.\n",
      "2025-06-01 14:30:16,494 - INFO - 335981393.py:862 - Calculating dFC_AbsDiffMean for S 002_S_0729 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:16,502 - INFO - 335981393.py:900 - dFC_AbsDiffMean for S 002_S_0729 calculated. Shape: (131, 131). Took 0.01s.\n",
      "2025-06-01 14:30:16,507 - INFO - 335981393.py:862 - Calculating ElasticNet_VAR_Influence for S 002_S_0729 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:17,802 - INFO - 335981393.py:728 - MI_KNN_Symmetric (S 002_S_2010): Successfully calculated.\n",
      "2025-06-01 14:30:17,803 - INFO - 335981393.py:900 - MI_KNN_Symmetric for S 002_S_2010 calculated. Shape: (131, 131). Took 21.72s.\n",
      "2025-06-01 14:30:17,804 - INFO - 335981393.py:862 - Calculating dFC_AbsDiffMean for S 002_S_2010 (TS shape: (140, 131))...\n",
      "2025-06-01 14:30:17,831 - INFO - 335981393.py:900 - dFC_AbsDiffMean for S 002_S_2010 calculated. Shape: (131, 131). Took 0.03s.\n",
      "2025-06-01 14:30:17,843 - INFO - 335981393.py:862 - Calculating ElasticNet_VAR_Influence for S 002_S_2010 (TS shape: (140, 131))...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Utilitario para Jupyter Notebook: Extracción de Características de Conectividad fMRI\n",
    "Versión: v6.5.3_ElasticNetRelaxed (Adaptado de v6.5.3_LassoRelaxed)\n",
    "\n",
    "Cambios Principales:\n",
    "- Reemplazado MultiTaskLassoCV con MultiTaskElasticNetCV para la estimación de conectividad VAR.\n",
    "- Añadido parámetro ENET_L1_RATIOS para controlar la mezcla de penalización L1/L2.\n",
    "- Actualizado el nombre del canal de conectividad VAR a 'ElasticNet_VAR_Influence'.\n",
    "- Ajustado el nombre del directorio de salida.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from nilearn.glm.first_level import spm_hrf, glover_hrf \n",
    "from scipy.signal import butter, filtfilt, deconvolve, windows\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from sklearn.linear_model import MultiTaskElasticNetCV # MODIFICADO: Importado MultiTaskElasticNetCV\n",
    "import networkx as nx \n",
    "import warnings \n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# --- Configuración del Logger ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Importación de OMST usando dyconnmap ---\n",
    "OMST_PYTHON_LOADED = False\n",
    "orthogonal_minimum_spanning_tree = None\n",
    "PEARSON_OMST_CHANNEL_NAME_PRIMARY = \"Pearson_OMST_GCE_Weighted\" \n",
    "PEARSON_OMST_FALLBACK_NAME = \"Pearson_Full_FisherZ\" \n",
    "PEARSON_OMST_CHANNEL_NAME = PEARSON_OMST_FALLBACK_NAME \n",
    "\n",
    "try:\n",
    "    from dyconnmap.graphs.threshold import threshold_omst_global_cost_efficiency\n",
    "    orthogonal_minimum_spanning_tree = threshold_omst_global_cost_efficiency \n",
    "    logger.info(\"Successfully imported 'threshold_omst_global_cost_efficiency' from 'dyconnmap.graphs.threshold' and aliased as 'orthogonal_minimum_spanning_tree'.\")\n",
    "    OMST_PYTHON_LOADED = True\n",
    "    PEARSON_OMST_CHANNEL_NAME = PEARSON_OMST_CHANNEL_NAME_PRIMARY \n",
    "except ImportError:\n",
    "    logger.error(\"ERROR: Dyconnmap module or 'threshold_omst_global_cost_efficiency' not found. \"\n",
    "                 f\"Channel '{PEARSON_OMST_FALLBACK_NAME}' will be used as fallback. \"\n",
    "                 \"Please ensure dyconnmap is installed: pip install dyconnmap\")\n",
    "except Exception as e_import: \n",
    "    logger.error(f\"ERROR during dyconnmap import: {e_import}. \"\n",
    "                 f\"Channel '{PEARSON_OMST_FALLBACK_NAME}' will be used as fallback.\")\n",
    "\n",
    "\n",
    "# --- 0. Global Configuration and Constants ---\n",
    "\n",
    "# --- Configurable Parameters ---\n",
    "BASE_PATH_AAL3 = Path('/home/diego/Escritorio/AAL3') \n",
    "QC_OUTPUT_DIR = BASE_PATH_AAL3 / 'qc_outputs_doctoral_v3.2_aal3_shrinkage_flexible_thresh_fix'\n",
    "SUBJECT_METADATA_CSV_PATH = BASE_PATH_AAL3 / 'SubjectsData_Schaefer2018_400ROIs.csv'\n",
    "QC_REPORT_CSV_PATH = QC_OUTPUT_DIR / 'report_qc_final_with_discard_flags_v3.2.csv'\n",
    "ROI_SIGNALS_DIR_PATH_AAL3 = BASE_PATH_AAL3 / 'ROISignals_AAL3_NiftiPreprocessedAllBatchesNorm'\n",
    "ROI_FILENAME_TEMPLATE = 'ROISignals_{subject_id}.mat'\n",
    "AAL3_META_PATH = BASE_PATH_AAL3 / 'ROI_MNI_V7_vol.txt' \n",
    "\n",
    "TR_SECONDS = 3.0 \n",
    "LOW_CUT_HZ = 0.01\n",
    "HIGH_CUT_HZ = 0.08\n",
    "FILTER_ORDER = 2 \n",
    "TAPER_ALPHA = 0.1 \n",
    "\n",
    "RAW_DATA_EXPECTED_COLUMNS = 170 \n",
    "AAL3_MISSING_INDICES_1BASED = [35, 36, 81, 82] \n",
    "EXPECTED_ROIS_AFTER_AAL3_MISSING_REMOVAL = RAW_DATA_EXPECTED_COLUMNS - len(AAL3_MISSING_INDICES_1BASED) \n",
    "SMALL_ROI_VOXEL_THRESHOLD = 100 \n",
    "\n",
    "N_ROIS_EXPECTED = 131 \n",
    "TARGET_LEN_TS = 140 \n",
    "\n",
    "N_NEIGHBORS_MI = 5 \n",
    "DFC_WIN_POINTS = 30 \n",
    "DFC_STEP = 5      \n",
    "VAR_MAX_LAG = 1 # Renombrado de LASSO_VAR_MAX_LAG\n",
    "APPLY_HRF_DECONVOLUTION = False \n",
    "HRF_MODEL = 'glover' \n",
    "\n",
    "# Ajustes para MultiTaskElasticNetCV\n",
    "ENET_MAX_ITER = 8000        \n",
    "ENET_TOL = 1e-3             \n",
    "ENET_ALPHAS = np.logspace(-5, -1, 20) # Rango de alphas más pequeños y más granular (0.00001 a 0.1)\n",
    "ENET_L1_RATIOS = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99] # L1 ratios para ElasticNetCV\n",
    "ENET_SELECTION = 'cyclic'   \n",
    "\n",
    "deconv_str = \"_deconv\" if APPLY_HRF_DECONVOLUTION else \"\"\n",
    "# El nombre del directorio se actualizará en _initialize_aal3_roi_processing_info\n",
    "OUTPUT_CONNECTIVITY_DIR_NAME = f\"AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.3_ElasticNetRelaxed\" \n",
    "\n",
    "POSSIBLE_ROI_KEYS = [\"signals\", \"ROISignals\", \"roi_signals\", \"ROIsignals_AAL3\", \"AAL3_signals\", \"roi_ts\"] \n",
    "\n",
    "USE_MI_CHANNEL_FOR_THESIS = True \n",
    "USE_DFC_CHANNEL_FOR_THESIS = True \n",
    "USE_VAR_CHANNEL_FOR_THESIS = True # Renombrado de USE_LASSO_VAR_FOR_THESIS\n",
    "\n",
    "CONNECTIVITY_CHANNEL_NAMES: List[str] = [] \n",
    "N_CHANNELS = 0 \n",
    "\n",
    "\n",
    "# --- Global AAL3 ROI Processing Variables ---\n",
    "VALID_AAL3_ROI_INFO_DF_166: Optional[pd.DataFrame] = None\n",
    "AAL3_MISSING_INDICES_0BASED: Optional[List[int]] = None\n",
    "INDICES_OF_SMALL_ROIS_TO_DROP_FROM_166: Optional[List[int]] = None\n",
    "FINAL_N_ROIS_EXPECTED: Optional[int] = None \n",
    "\n",
    "def _initialize_aal3_roi_processing_info():\n",
    "    global VALID_AAL3_ROI_INFO_DF_166, AAL3_MISSING_INDICES_0BASED, \\\n",
    "           INDICES_OF_SMALL_ROIS_TO_DROP_FROM_166, FINAL_N_ROIS_EXPECTED, \\\n",
    "           N_ROIS_EXPECTED, OUTPUT_CONNECTIVITY_DIR_NAME, CONNECTIVITY_CHANNEL_NAMES, N_CHANNELS, \\\n",
    "           PEARSON_OMST_CHANNEL_NAME \n",
    "\n",
    "    logger.info(\"--- Initializing AAL3 ROI Processing Information ---\")\n",
    "    \n",
    "    omst_suffix_for_dir = \"OMST_GCE\" if OMST_PYTHON_LOADED and orthogonal_minimum_spanning_tree is not None else \"PearsonFull\"\n",
    "    current_pearson_channel = PEARSON_OMST_CHANNEL_NAME \n",
    "\n",
    "    if not AAL3_META_PATH.exists():\n",
    "        logger.error(f\"AAL3 metadata file NOT found: {AAL3_META_PATH}. Cannot perform ROI reduction. \"\n",
    "                     f\"Using placeholder N_ROIS_EXPECTED = {N_ROIS_EXPECTED}.\")\n",
    "        FINAL_N_ROIS_EXPECTED = N_ROIS_EXPECTED \n",
    "        OUTPUT_CONNECTIVITY_DIR_NAME = f\"AAL3_{N_ROIS_EXPECTED}ROIs_fmri_tensor_{omst_suffix_for_dir}_{VAR_MAX_LAG}lag{deconv_str}_NeuroEnhanced_v6.5.3_ElasticNetRelaxed_ERR\"\n",
    "    else:\n",
    "        try:\n",
    "            meta_aal3_df = pd.read_csv(AAL3_META_PATH, sep='\\t')\n",
    "            meta_aal3_df['color'] = pd.to_numeric(meta_aal3_df['color'], errors='coerce')\n",
    "            meta_aal3_df.dropna(subset=['color'], inplace=True)\n",
    "            meta_aal3_df['color'] = meta_aal3_df['color'].astype(int)\n",
    "            \n",
    "            if not all(col in meta_aal3_df.columns for col in ['nom_c', 'color', 'vol_vox']):\n",
    "                raise ValueError(\"AAL3 metadata must contain 'nom_c', 'color', 'vol_vox'.\")\n",
    "\n",
    "            AAL3_MISSING_INDICES_0BASED = [idx - 1 for idx in AAL3_MISSING_INDICES_1BASED]\n",
    "            VALID_AAL3_ROI_INFO_DF_166 = meta_aal3_df[~meta_aal3_df['color'].isin(AAL3_MISSING_INDICES_1BASED)].copy()\n",
    "            VALID_AAL3_ROI_INFO_DF_166.sort_values(by='color', inplace=True)\n",
    "            VALID_AAL3_ROI_INFO_DF_166.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            if len(VALID_AAL3_ROI_INFO_DF_166) != EXPECTED_ROIS_AFTER_AAL3_MISSING_REMOVAL:\n",
    "                logger.warning(f\"Expected {EXPECTED_ROIS_AFTER_AAL3_MISSING_REMOVAL} ROIs in AAL3 meta after filtering known missing, \"\n",
    "                               f\"but found {len(VALID_AAL3_ROI_INFO_DF_166)}. Check AAL3_META_PATH content and AAL3_MISSING_INDICES_1BASED.\")\n",
    "            \n",
    "            small_rois_mask_on_166 = VALID_AAL3_ROI_INFO_DF_166['vol_vox'] < SMALL_ROI_VOXEL_THRESHOLD\n",
    "            INDICES_OF_SMALL_ROIS_TO_DROP_FROM_166 = VALID_AAL3_ROI_INFO_DF_166[small_rois_mask_on_166].index.tolist()\n",
    "            \n",
    "            FINAL_N_ROIS_EXPECTED = EXPECTED_ROIS_AFTER_AAL3_MISSING_REMOVAL - len(INDICES_OF_SMALL_ROIS_TO_DROP_FROM_166)\n",
    "            N_ROIS_EXPECTED = FINAL_N_ROIS_EXPECTED \n",
    "            \n",
    "            OUTPUT_CONNECTIVITY_DIR_NAME = f\"AAL3_{N_ROIS_EXPECTED}ROIs_fmri_tensor_{omst_suffix_for_dir}_{VAR_MAX_LAG}lag{deconv_str}_NeuroEnhanced_v6.5.3_ElasticNetRelaxed\"\n",
    "\n",
    "            logger.info(f\"AAL3 ROI processing info initialized:\")\n",
    "            logger.info(f\"  Indices of 4 AAL3 systemically missing ROIs (0-based, from 170): {AAL3_MISSING_INDICES_0BASED}\")\n",
    "            logger.info(f\"  Number of ROIs in AAL3 meta after excluding systemically missing: {len(VALID_AAL3_ROI_INFO_DF_166)} (Expected: {EXPECTED_ROIS_AFTER_AAL3_MISSING_REMOVAL})\")\n",
    "            logger.info(f\"  Indices of small ROIs to drop (from the {len(VALID_AAL3_ROI_INFO_DF_166)} set, 0-based): {INDICES_OF_SMALL_ROIS_TO_DROP_FROM_166}\")\n",
    "            logger.info(f\"  Number of small ROIs to drop: {len(INDICES_OF_SMALL_ROIS_TO_DROP_FROM_166)}\")\n",
    "            logger.info(f\"  FINAL_N_ROIS_EXPECTED for connectivity analysis: {FINAL_N_ROIS_EXPECTED} (This should be 131 if matching QC script)\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing AAL3 ROI processing info: {e}\", exc_info=True)\n",
    "            FINAL_N_ROIS_EXPECTED = N_ROIS_EXPECTED \n",
    "            OUTPUT_CONNECTIVITY_DIR_NAME = f\"AAL3_{N_ROIS_EXPECTED}ROIs_fmri_tensor_{omst_suffix_for_dir}_{VAR_MAX_LAG}lag{deconv_str}_NeuroEnhanced_v6.5.3_ElasticNetRelaxed_ERROR_INIT\"\n",
    "            \n",
    "    temp_channels = [current_pearson_channel] \n",
    "    if not (OMST_PYTHON_LOADED and orthogonal_minimum_spanning_tree is not None) and current_pearson_channel == PEARSON_OMST_CHANNEL_NAME_PRIMARY:\n",
    "         logger.warning(f\"OMST function from dyconnmap not loaded or is None, but primary OMST channel name was set. \"\n",
    "                        f\"The channel '{PEARSON_OMST_CHANNEL_NAME_PRIMARY}' will effectively be '{PEARSON_OMST_FALLBACK_NAME}'.\")\n",
    "    elif not (OMST_PYTHON_LOADED and orthogonal_minimum_spanning_tree is not None):\n",
    "         logger.info(f\"OMST function from dyconnmap not loaded or is None. Using '{PEARSON_OMST_FALLBACK_NAME}' as the first channel name in list.\")\n",
    "\n",
    "    if USE_MI_CHANNEL_FOR_THESIS: temp_channels.append(\"MI_KNN_Symmetric\")\n",
    "    if USE_DFC_CHANNEL_FOR_THESIS: temp_channels.append(\"dFC_AbsDiffMean\")\n",
    "    if USE_VAR_CHANNEL_FOR_THESIS: # MODIFICADO\n",
    "        var_channel_suffix_dynamic = \"_Deconv_Influence\" if APPLY_HRF_DECONVOLUTION else \"_Influence\"\n",
    "        temp_channels.append(f\"ElasticNet_VAR{var_channel_suffix_dynamic}\") # MODIFICADO: Nombre del canal\n",
    "    \n",
    "    CONNECTIVITY_CHANNEL_NAMES = temp_channels\n",
    "    N_CHANNELS = len(CONNECTIVITY_CHANNEL_NAMES)\n",
    "    return True\n",
    "\n",
    "\n",
    "if not _initialize_aal3_roi_processing_info():\n",
    "    logger.warning(\"ROI processing info could not be initialized properly. Pipeline may use placeholder values or fail for some operations.\")\n",
    "\n",
    "logger.info(f\"Final N_ROIS_EXPECTED after initialization: {N_ROIS_EXPECTED}\")\n",
    "logger.info(f\"Final OUTPUT_CONNECTIVITY_DIR_NAME: {OUTPUT_CONNECTIVITY_DIR_NAME}\")\n",
    "logger.info(f\"Connectivity channels to be computed: {CONNECTIVITY_CHANNEL_NAMES}\") \n",
    "logger.info(f\"Total number of channels (for VAE): {N_CHANNELS}\")\n",
    "\n",
    "\n",
    "# --- 1. Subject Metadata Loading and Merging ---\n",
    "def load_metadata(\n",
    "    subject_meta_csv_path: Path,\n",
    "    qc_report_csv_path: Path\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    # ... (código sin cambios)\n",
    "    logger.info(\"--- Starting Subject Metadata Loading and QC Integration ---\")\n",
    "    try:\n",
    "        if not subject_meta_csv_path.exists():\n",
    "            logger.critical(f\"Subject metadata CSV file NOT found: {subject_meta_csv_path}\")\n",
    "            return None\n",
    "        if not qc_report_csv_path.exists():\n",
    "            logger.critical(f\"QC report CSV file NOT found: {qc_report_csv_path}\")\n",
    "            return None\n",
    "\n",
    "        subjects_db_df = pd.read_csv(subject_meta_csv_path)\n",
    "        subjects_db_df['SubjectID'] = subjects_db_df['SubjectID'].astype(str).str.strip() \n",
    "        logger.info(f\"Loaded main metadata from {subject_meta_csv_path}. Shape: {subjects_db_df.shape}\")\n",
    "        if 'SubjectID' not in subjects_db_df.columns:\n",
    "            logger.critical(\"Column 'SubjectID' missing in main metadata CSV.\")\n",
    "            return None\n",
    "        if 'ResearchGroup' not in subjects_db_df.columns:\n",
    "            logger.warning(\"Column 'ResearchGroup' missing in main metadata CSV. May be needed for downstream VAE tasks.\")\n",
    "\n",
    "        qc_df = pd.read_csv(qc_report_csv_path)\n",
    "        logger.info(f\"Loaded QC report from {qc_report_csv_path}. Shape: {qc_df.shape}\")\n",
    "\n",
    "        if 'Subject' in qc_df.columns and 'SubjectID' not in qc_df.columns:\n",
    "            logger.info(\"Found 'Subject' column in QC report, renaming to 'SubjectID'.\")\n",
    "            qc_df.rename(columns={'Subject': 'SubjectID'}, inplace=True)\n",
    "        \n",
    "        if 'SubjectID' in qc_df.columns:\n",
    "            qc_df['SubjectID'] = qc_df['SubjectID'].astype(str).str.strip()\n",
    "        else:\n",
    "            logger.critical(\"Neither 'Subject' nor 'SubjectID' column found in QC report CSV.\")\n",
    "            return None\n",
    "        \n",
    "        essential_qc_cols = ['SubjectID', 'ToDiscard_Overall', 'TimePoints']\n",
    "        if not all(col in qc_df.columns for col in essential_qc_cols):\n",
    "            logger.critical(f\"Essential columns ({essential_qc_cols}) missing in QC report CSV.\")\n",
    "            return None\n",
    "\n",
    "        merged_df = pd.merge(subjects_db_df, qc_df, on='SubjectID', how='inner', suffixes=('_meta', '_qc'))\n",
    "        \n",
    "        if 'TimePoints_qc' in merged_df.columns: \n",
    "            merged_df['Timepoints_final_for_script'] = merged_df['TimePoints_qc']\n",
    "        elif 'TimePoints' in merged_df.columns: \n",
    "             merged_df['Timepoints_final_for_script'] = merged_df['TimePoints']\n",
    "        else: \n",
    "            logger.critical(\"Definitive 'TimePoints' column from QC report could not be identified after merge.\")\n",
    "            return None\n",
    "        \n",
    "        merged_df['Timepoints_final_for_script'] = pd.to_numeric(merged_df['Timepoints_final_for_script'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        initial_subject_count = len(merged_df)\n",
    "        subjects_passing_qc_df = merged_df[merged_df['ToDiscard_Overall'] == False].copy()\n",
    "        num_discarded = initial_subject_count - len(subjects_passing_qc_df)\n",
    "        \n",
    "        logger.info(f\"Total subjects after merge: {initial_subject_count}\")\n",
    "        logger.info(f\"Subjects discarded based on QC ('ToDiscard_Overall' == True): {num_discarded}\")\n",
    "        logger.info(f\"Subjects passing QC and to be processed: {len(subjects_passing_qc_df)}\")\n",
    "\n",
    "        if subjects_passing_qc_df.empty:\n",
    "            logger.warning(\"No subjects passed QC. Check your QC criteria and report.\")\n",
    "            return None\n",
    "            \n",
    "        min_tp_after_qc = subjects_passing_qc_df['Timepoints_final_for_script'].min()\n",
    "        max_tp_after_qc = subjects_passing_qc_df['Timepoints_final_for_script'].max()\n",
    "        logger.info(f\"Timepoints for subjects passing QC (from QC report): Min={min_tp_after_qc}, Max={max_tp_after_qc}.\")\n",
    "        logger.info(f\"These will be homogenized to TARGET_LEN_TS = {TARGET_LEN_TS} for connectivity calculation.\")\n",
    "\n",
    "        final_cols_to_keep = ['SubjectID']\n",
    "        subjects_passing_qc_df.rename(columns={'Timepoints_final_for_script': 'Timepoints'}, inplace=True)\n",
    "        final_cols_to_keep.append('Timepoints')\n",
    "\n",
    "        if 'ResearchGroup_meta' in subjects_passing_qc_df.columns: \n",
    "            subjects_passing_qc_df.rename(columns={'ResearchGroup_meta': 'ResearchGroup'}, inplace=True)\n",
    "        elif 'ResearchGroup_qc' in subjects_passing_qc_df.columns and 'ResearchGroup' not in subjects_passing_qc_df.columns:\n",
    "            subjects_passing_qc_df.rename(columns={'ResearchGroup_qc': 'ResearchGroup'}, inplace=True)\n",
    "        \n",
    "        if 'ResearchGroup' in subjects_passing_qc_df.columns:\n",
    "             final_cols_to_keep.append('ResearchGroup')\n",
    "        else:\n",
    "            logger.warning(\"Creating placeholder 'ResearchGroup' column as it was not found. This is important for classification.\")\n",
    "            subjects_passing_qc_df['ResearchGroup'] = 'Unknown' \n",
    "            final_cols_to_keep.append('ResearchGroup')\n",
    "        \n",
    "        final_cols_to_keep = list(dict.fromkeys(final_cols_to_keep)) \n",
    "        return subjects_passing_qc_df[final_cols_to_keep]\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logger.critical(f\"CRITICAL Error loading CSV files: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        logger.critical(f\"Value error in metadata processing: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Unexpected error during metadata loading/QC integration: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "# --- 2. Time Series Loading and Preprocessing Functions ---\n",
    "def _load_signals_from_mat(mat_path: Path, possible_keys: List[str]) -> Optional[np.ndarray]:\n",
    "    # ... (código sin cambios) ...\n",
    "    try:\n",
    "        data = sio.loadmat(mat_path)\n",
    "    except Exception as e_load:\n",
    "        logger.error(f\"Could not load .mat file: {mat_path}. Error: {e_load}\")\n",
    "        return None\n",
    "    \n",
    "    for key in possible_keys:\n",
    "        if key in data and isinstance(data[key], np.ndarray) and data[key].ndim >= 2:\n",
    "            logger.debug(f\"Found signals under key '{key}' in {mat_path.name}. Shape: {data[key].shape}\")\n",
    "            return data[key].astype(np.float64) \n",
    "            \n",
    "    logger.warning(f\"No valid signal keys {possible_keys} found in {mat_path.name}. Keys present: {list(data.keys())}\")\n",
    "    return None\n",
    "\n",
    "def _orient_and_reduce_rois(\n",
    "    raw_sigs: np.ndarray, \n",
    "    subject_id: str,\n",
    "    initial_expected_cols: int, \n",
    "    aal3_missing_0based: Optional[List[int]], \n",
    "    small_rois_indices_from_166: Optional[List[int]], \n",
    "    final_expected_rois: Optional[int] \n",
    ") -> Optional[np.ndarray]:\n",
    "    # ... (código sin cambios) ...\n",
    "    if raw_sigs.ndim != 2:\n",
    "        logger.warning(f\"S {subject_id}: Raw signal matrix has incorrect dimensions {raw_sigs.ndim} (expected 2). Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    oriented_sigs = raw_sigs.copy()\n",
    "    if oriented_sigs.shape[0] == initial_expected_cols and oriented_sigs.shape[1] != initial_expected_cols:\n",
    "        logger.info(f\"S {subject_id}: Transposing raw matrix from {oriented_sigs.shape} to ({oriented_sigs.shape[1]}, {oriented_sigs.shape[0]}) to match (TPs, ROIs_initial).\")\n",
    "        oriented_sigs = oriented_sigs.T\n",
    "    elif oriented_sigs.shape[1] == initial_expected_cols and oriented_sigs.shape[0] != initial_expected_cols:\n",
    "        logger.debug(f\"S {subject_id}: Raw matrix already (TPs, ROIs_initial): {oriented_sigs.shape}.\") \n",
    "    elif oriented_sigs.shape[0] == initial_expected_cols and oriented_sigs.shape[1] == initial_expected_cols:\n",
    "         logger.warning(f\"S {subject_id}: Raw signal matrix is square ({oriented_sigs.shape}) and matches initial_expected_cols. Assuming [Timepoints, ROIs_initial]. Careful if TPs also equals initial_expected_cols.\")\n",
    "    else: \n",
    "        logger.warning(f\"S {subject_id}: Neither dimension of raw signal matrix ({oriented_sigs.shape}) matches initial_expected_cols ({initial_expected_cols}). Skipping.\")\n",
    "        return None\n",
    "\n",
    "    if oriented_sigs.shape[1] != initial_expected_cols: \n",
    "        logger.warning(f\"S {subject_id}: After orientation, raw ROI count ({oriented_sigs.shape[1]}) != initial_expected_cols ({initial_expected_cols}). Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    if aal3_missing_0based is None:\n",
    "        logger.warning(f\"S {subject_id}: AAL3 missing ROI indices (0-based) not available. Skipping AAL3 known missing ROI removal. Using {oriented_sigs.shape[1]} ROIs for next step.\")\n",
    "        sigs_after_known_missing_removed = oriented_sigs \n",
    "    else:\n",
    "        try:\n",
    "            sigs_after_known_missing_removed = np.delete(oriented_sigs, aal3_missing_0based, axis=1)\n",
    "            logger.info(f\"S {subject_id}: Removed {len(aal3_missing_0based)} known missing AAL3 ROIs. Shape {oriented_sigs.shape} -> {sigs_after_known_missing_removed.shape}\")\n",
    "            if sigs_after_known_missing_removed.shape[1] != EXPECTED_ROIS_AFTER_AAL3_MISSING_REMOVAL:\n",
    "                 logger.warning(f\"S {subject_id}: After removing known missing ROIs, shape is {sigs_after_known_missing_removed.shape}, but expected (..., {EXPECTED_ROIS_AFTER_AAL3_MISSING_REMOVAL}).\")\n",
    "        except IndexError as e:\n",
    "            logger.error(f\"S {subject_id}: IndexError removing known missing AAL3 ROIs (indices: {aal3_missing_0based}) from matrix of shape {oriented_sigs.shape}. Error: {e}. Using original {oriented_sigs.shape[1]} ROIs for next step.\")\n",
    "            sigs_after_known_missing_removed = oriented_sigs \n",
    "            \n",
    "    if small_rois_indices_from_166 is None:\n",
    "        logger.warning(f\"S {subject_id}: Small ROI indices (from 166-set) not available. Skipping small ROI removal. Using {sigs_after_known_missing_removed.shape[1]} ROIs.\")\n",
    "        sigs_final_rois = sigs_after_known_missing_removed\n",
    "    elif sigs_after_known_missing_removed.shape[1] != EXPECTED_ROIS_AFTER_AAL3_MISSING_REMOVAL:\n",
    "        logger.warning(f\"S {subject_id}: Cannot remove small ROIs because the matrix (shape {sigs_after_known_missing_removed.shape}) does not have the expected {EXPECTED_ROIS_AFTER_AAL3_MISSING_REMOVAL} columns after first reduction step. Using current ROIs ({sigs_after_known_missing_removed.shape[1]}).\")\n",
    "        sigs_final_rois = sigs_after_known_missing_removed\n",
    "    else:\n",
    "        try:\n",
    "            sigs_final_rois = np.delete(sigs_after_known_missing_removed, small_rois_indices_from_166, axis=1)\n",
    "            logger.info(f\"S {subject_id}: Removed {len(small_rois_indices_from_166)} small ROIs. Shape {sigs_after_known_missing_removed.shape} -> {sigs_final_rois.shape}\")\n",
    "        except IndexError as e:\n",
    "            logger.error(f\"S {subject_id}: IndexError removing small ROIs (indices: {small_rois_indices_from_166}) from matrix of shape {sigs_after_known_missing_removed.shape}. Error: {e}. Using {sigs_after_known_missing_removed.shape[1]} ROIs.\")\n",
    "            sigs_final_rois = sigs_after_known_missing_removed \n",
    "\n",
    "    if final_expected_rois is not None and sigs_final_rois.shape[1] != final_expected_rois:\n",
    "        logger.warning(f\"S {subject_id}: Final ROI count ({sigs_final_rois.shape[1]}) != FINAL_N_ROIS_EXPECTED ({final_expected_rois}). \"\n",
    "                       \"This may indicate issues in AAL3 metadata or reduction logic. Proceeding with current matrix.\")\n",
    "    elif final_expected_rois is None:\n",
    "        logger.warning(f\"S {subject_id}: FINAL_N_ROIS_EXPECTED is None. Cannot validate final ROI count. Proceeding with {sigs_final_rois.shape[1]} ROIs.\")\n",
    "    else:\n",
    "        logger.info(f\"S {subject_id}: Final ROI count {sigs_final_rois.shape[1]} matches FINAL_N_ROIS_EXPECTED {final_expected_rois}.\")\n",
    "        \n",
    "    return sigs_final_rois\n",
    "\n",
    "\n",
    "def _bandpass_filter_signals(sigs: np.ndarray, lowcut: float, highcut: float, fs: float, order: int, subject_id: str, taper_alpha: float = 0.1) -> np.ndarray:\n",
    "    # ... (código sin cambios) ...\n",
    "    nyquist_freq = 0.5 * fs\n",
    "    low_norm = lowcut / nyquist_freq\n",
    "    high_norm = highcut / nyquist_freq\n",
    "\n",
    "    if not (0 < low_norm < 1 and 0 < high_norm < 1 and low_norm < high_norm):\n",
    "        logger.error(f\"S {subject_id}: Invalid critical frequencies for filter (low_norm={low_norm}, high_norm={high_norm}). Nyquist={nyquist_freq}. Skipping filtering.\")\n",
    "        return sigs\n",
    "    try:\n",
    "        b, a = butter(order, [low_norm, high_norm], btype='band', analog=False)\n",
    "        filtered_sigs = np.zeros_like(sigs)\n",
    "        padlen_required = 3 * (max(len(a), len(b))) \n",
    "        \n",
    "        for i in range(sigs.shape[1]): \n",
    "            roi_signal = sigs[:, i].copy() \n",
    "            \n",
    "            if len(roi_signal) > padlen_required: \n",
    "                try:\n",
    "                    tukey_window = windows.tukey(len(roi_signal), alpha=taper_alpha)\n",
    "                    roi_signal_tapered = roi_signal * tukey_window\n",
    "                except Exception as e_taper:\n",
    "                    logger.warning(f\"S {subject_id}, ROI {i}: Error applying Tukey window: {e_taper}. Proceeding without taper.\")\n",
    "                    roi_signal_tapered = roi_signal \n",
    "            else:\n",
    "                roi_signal_tapered = roi_signal \n",
    "\n",
    "            if np.all(np.isclose(roi_signal_tapered, roi_signal_tapered[0] if len(roi_signal_tapered)>0 else 0.0)): \n",
    "                logger.debug(f\"S {subject_id}, ROI {i}: Signal is constant (possibly after taper). Skipping filter.\")\n",
    "                filtered_sigs[:, i] = roi_signal_tapered \n",
    "            elif len(roi_signal_tapered) <= padlen_required :\n",
    "                logger.warning(f\"S {subject_id}, ROI {i}: Signal too short ({len(roi_signal_tapered)} pts, need > {padlen_required}) for filtfilt. Skipping filter for this ROI.\")\n",
    "                filtered_sigs[:, i] = roi_signal_tapered\n",
    "            else:\n",
    "                filtered_sigs[:, i] = filtfilt(b, a, roi_signal_tapered)\n",
    "        return filtered_sigs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"S {subject_id}: Error during bandpass filtering: {e}. Returning original signals.\", exc_info=False)\n",
    "        return sigs\n",
    "\n",
    "def _hrf_deconvolution(sigs: np.ndarray, tr: float, hrf_model_type: str, subject_id: str) -> np.ndarray:\n",
    "    # ... (código sin cambios) ...\n",
    "    logger.info(f\"S {subject_id}: Attempting HRF deconvolution (Model: {hrf_model_type}, TR: {tr}s).\")\n",
    "    if hrf_model_type == 'glover': \n",
    "        hrf_kernel = glover_hrf(tr, oversampling=1) \n",
    "    elif hrf_model_type == 'spm': \n",
    "        hrf_kernel = spm_hrf(tr, oversampling=1)\n",
    "    else: \n",
    "        logger.error(f\"S {subject_id}: Unknown HRF model type '{hrf_model_type}'. Skipping deconvolution.\")\n",
    "        return sigs\n",
    "\n",
    "    if len(hrf_kernel) == 0 or np.all(np.isclose(hrf_kernel, 0)):\n",
    "        logger.error(f\"S {subject_id}: HRF kernel is empty or all zeros for model '{hrf_model_type}'. Skipping deconvolution.\")\n",
    "        return sigs\n",
    "\n",
    "    deconvolved_sigs = np.zeros_like(sigs)\n",
    "    for i in range(sigs.shape[1]): \n",
    "        signal_roi = sigs[:, i]\n",
    "        if len(signal_roi) < len(hrf_kernel): \n",
    "            logger.warning(f\"S {subject_id}, ROI {i}: Signal length ({len(signal_roi)}) is shorter than HRF kernel length ({len(hrf_kernel)}). Skipping deconvolution for this ROI.\")\n",
    "            deconvolved_sigs[:, i] = signal_roi\n",
    "            continue\n",
    "        try:\n",
    "            quotient, _ = deconvolve(signal_roi, hrf_kernel)\n",
    "            if len(quotient) < sigs.shape[0]:\n",
    "                deconvolved_sigs[:, i] = np.concatenate([quotient, np.zeros(sigs.shape[0] - len(quotient))])\n",
    "            else: \n",
    "                deconvolved_sigs[:, i] = quotient[:sigs.shape[0]]\n",
    "        except Exception as e_deconv:\n",
    "            logger.error(f\"S {subject_id}, ROI {i}: Deconvolution failed: {e_deconv}. Using original signal for this ROI.\", exc_info=False)\n",
    "            deconvolved_sigs[:, i] = signal_roi\n",
    "            \n",
    "    logger.info(f\"S {subject_id}: HRF deconvolution attempt finished.\")\n",
    "    return deconvolved_sigs\n",
    "\n",
    "def _preprocess_time_series(\n",
    "    sigs: np.ndarray, \n",
    "    target_len_ts_val: int, \n",
    "    subject_id: str, \n",
    "    var_max_lag_val: int, # MODIFICADO: Renombrado\n",
    "    tr_seconds_val: float, low_cut_val: float, high_cut_val: float, filter_order_val: int,\n",
    "    apply_hrf_deconv_val: bool, hrf_model_type_val: str,\n",
    "    taper_alpha_val: float \n",
    ") -> Optional[np.ndarray]:\n",
    "    # ... (código con interpolación y taper ya implementado) ...\n",
    "    original_length, current_n_rois = sigs.shape\n",
    "    fs = 1.0 / tr_seconds_val \n",
    "    \n",
    "    logger.info(f\"S {subject_id}: Preprocessing. Input TPs: {original_length}, ROIs: {current_n_rois} (should be {FINAL_N_ROIS_EXPECTED}), TR: {tr_seconds_val}s. Target TPs for output: {target_len_ts_val}.\")\n",
    "    \n",
    "    sigs_processed = _bandpass_filter_signals(sigs, low_cut_val, high_cut_val, fs, filter_order_val, subject_id, taper_alpha=taper_alpha_val)\n",
    "    \n",
    "    if apply_hrf_deconv_val:\n",
    "        sigs_processed = _hrf_deconvolution(sigs_processed, tr_seconds_val, hrf_model_type_val, subject_id)\n",
    "        if np.isnan(sigs_processed).any() or np.isinf(sigs_processed).any():\n",
    "            logger.warning(f\"S {subject_id}: NaNs/Infs detected after HRF deconvolution. Cleaning by replacing with 0.0.\")\n",
    "            sigs_processed = np.nan_to_num(sigs_processed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "    min_len_for_var = var_max_lag_val + 10 # MODIFICADO: Renombrado \n",
    "    min_len_for_dfc = DFC_WIN_POINTS if DFC_WIN_POINTS > 0 else 5 \n",
    "    min_overall_len = max(5, min_len_for_var, min_len_for_dfc) \n",
    "    if sigs_processed.shape[0] < min_overall_len:\n",
    "        logger.warning(f\"S {subject_id}: Timepoints after processing ({sigs_processed.shape[0]}) are less than minimum required ({min_overall_len}) for all connectivity measures. Skipping subject.\")\n",
    "        return None\n",
    "        \n",
    "    if np.isnan(sigs_processed).any():\n",
    "        logger.warning(f\"S {subject_id}: NaNs detected in signals before scaling. Filling with 0.0. This might affect results.\")\n",
    "        sigs_processed = np.nan_to_num(sigs_processed, nan=0.0) \n",
    "        \n",
    "    try:\n",
    "        scaler = StandardScaler() \n",
    "        sigs_normalized = scaler.fit_transform(sigs_processed)\n",
    "        if np.isnan(sigs_normalized).any(): \n",
    "            logger.warning(f\"S {subject_id}: NaNs detected after StandardScaler. Filling with 0.0. This is unusual.\")\n",
    "            sigs_normalized = np.nan_to_num(sigs_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    except ValueError as e_scale: \n",
    "        logger.warning(f\"S {subject_id}: StandardScaler failed (e.g. all-zero data after processing): {e_scale}. Attempting column-wise scaling or zeroing.\")\n",
    "        sigs_normalized = np.zeros_like(sigs_processed, dtype=np.float32)\n",
    "        for i in range(sigs_processed.shape[1]):\n",
    "            col_data = sigs_processed[:, i].reshape(-1,1)\n",
    "            if np.std(col_data) > 1e-9: \n",
    "                try: \n",
    "                    sigs_normalized[:, i] = StandardScaler().fit_transform(col_data).flatten()\n",
    "                except Exception as e_col_scale:\n",
    "                    logger.error(f\"S {subject_id}, ROI {i}: Column-wise scaling failed: {e_col_scale}. Setting to zero.\")\n",
    "                    sigs_normalized[:, i] = 0.0 \n",
    "            else: \n",
    "                sigs_normalized[:, i] = 0.0 \n",
    "        if np.isnan(sigs_normalized).any(): \n",
    "            sigs_normalized = np.nan_to_num(sigs_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    current_length_norm, num_rois_norm = sigs_normalized.shape\n",
    "    if current_length_norm != target_len_ts_val:\n",
    "        logger.info(f\"S {subject_id}: Homogenizing time series length from {current_length_norm} to {target_len_ts_val}.\")\n",
    "        if current_length_norm < target_len_ts_val:\n",
    "            logger.debug(f\"S {subject_id}: Interpolating from {current_length_norm} to {target_len_ts_val} points.\")\n",
    "            sigs_homogenized = np.zeros((target_len_ts_val, num_rois_norm), dtype=np.float32)\n",
    "            if current_length_norm > 1: \n",
    "                x_old = np.linspace(0, 1, current_length_norm)\n",
    "                x_new = np.linspace(0, 1, target_len_ts_val)\n",
    "                for i in range(num_rois_norm):\n",
    "                    f_interp = interp1d(x_old, sigs_normalized[:, i], kind='linear', fill_value=\"extrapolate\")\n",
    "                    sigs_homogenized[:, i] = f_interp(x_new)\n",
    "            elif current_length_norm == 1: \n",
    "                 for i in range(num_rois_norm):\n",
    "                    sigs_homogenized[:,i] = sigs_normalized[0,i] \n",
    "\n",
    "            if np.isnan(sigs_homogenized).any(): \n",
    "                logger.warning(f\"S {subject_id}: NaNs found after interpolation/length adjustment. Filling with 0.0.\")\n",
    "                sigs_homogenized = np.nan_to_num(sigs_homogenized, nan=0.0)\n",
    "        else: \n",
    "            sigs_homogenized = sigs_normalized[:target_len_ts_val, :]\n",
    "    else:\n",
    "        sigs_homogenized = sigs_normalized \n",
    "        \n",
    "    return sigs_homogenized.astype(np.float32)\n",
    "\n",
    "def load_and_preprocess_single_subject_series(\n",
    "    subject_id: str, \n",
    "    target_len_ts_val: int,\n",
    "    current_roi_signals_dir_path: Path, current_roi_filename_template: str,\n",
    "    possible_roi_keys_list: List[str], \n",
    "    var_max_lag_val: int, # MODIFICADO: Renombrado\n",
    "    tr_seconds_val: float, low_cut_val: float, high_cut_val: float, filter_order_val: int,\n",
    "    apply_hrf_deconv_val: bool, hrf_model_type_val: str,\n",
    "    taper_alpha_val: float \n",
    ") -> Tuple[Optional[np.ndarray], str, bool]:\n",
    "    # ... (código sin cambios) ...\n",
    "    mat_path = current_roi_signals_dir_path / current_roi_filename_template.format(subject_id=subject_id)\n",
    "    if not mat_path.exists(): \n",
    "        return None, f\"MAT file not found: {mat_path.name}\", False\n",
    "    \n",
    "    try:\n",
    "        loaded_sigs_raw_170 = _load_signals_from_mat(mat_path, possible_roi_keys_list)\n",
    "        if loaded_sigs_raw_170 is None: \n",
    "            return None, f\"No valid signal keys or load error in {mat_path.name}\", False\n",
    "        \n",
    "        sigs_reduced_rois = _orient_and_reduce_rois(\n",
    "            loaded_sigs_raw_170, subject_id, \n",
    "            RAW_DATA_EXPECTED_COLUMNS, \n",
    "            AAL3_MISSING_INDICES_0BASED, \n",
    "            INDICES_OF_SMALL_ROIS_TO_DROP_FROM_166, \n",
    "            FINAL_N_ROIS_EXPECTED \n",
    "        )\n",
    "        del loaded_sigs_raw_170; gc.collect() \n",
    "        if sigs_reduced_rois is None: \n",
    "            return None, f\"ROI orientation, reduction, or validation failed for S {subject_id}.\", False\n",
    "        \n",
    "        if FINAL_N_ROIS_EXPECTED is not None and sigs_reduced_rois.shape[1] != FINAL_N_ROIS_EXPECTED:\n",
    "            error_msg = (f\"S {subject_id}: Post-reduction ROI count ({sigs_reduced_rois.shape[1]}) \"\n",
    "                         f\"does not match FINAL_N_ROIS_EXPECTED ({FINAL_N_ROIS_EXPECTED}). This is unexpected. \"\n",
    "                         \"Check AAL3 metadata and ROI reduction logic.\")\n",
    "            logger.error(error_msg)\n",
    "            return None, error_msg, False\n",
    "        elif FINAL_N_ROIS_EXPECTED is None:\n",
    "             logger.warning(f\"S {subject_id}: FINAL_N_ROIS_EXPECTED is None, cannot strictly validate ROI count. Proceeding with {sigs_reduced_rois.shape[1]} ROIs.\")\n",
    "\n",
    "        original_tp_count = sigs_reduced_rois.shape[0]\n",
    "        \n",
    "        sigs_processed = _preprocess_time_series(\n",
    "            sigs_reduced_rois, target_len_ts_val,\n",
    "            subject_id, var_max_lag_val, # MODIFICADO: Renombrado\n",
    "            tr_seconds_val, low_cut_val, high_cut_val, filter_order_val,\n",
    "            apply_hrf_deconv_val, hrf_model_type_val,\n",
    "            taper_alpha_val=taper_alpha_val \n",
    "        )\n",
    "        del sigs_reduced_rois; gc.collect() \n",
    "        if sigs_processed is None: \n",
    "            return None, f\"Preprocessing (filtering, scaling, or length adjustment) failed for S {subject_id}. Original TPs: {original_tp_count}\", False\n",
    "        \n",
    "        final_shape_str = f\"({sigs_processed.shape[0]}, {sigs_processed.shape[1]})\"\n",
    "        if FINAL_N_ROIS_EXPECTED is not None and sigs_processed.shape[1] != FINAL_N_ROIS_EXPECTED:\n",
    "            error_msg = (f\"S {subject_id}: Processed signal ROI count ({sigs_processed.shape[1]}) \"\n",
    "                         f\"mismatches FINAL_N_ROIS_EXPECTED ({FINAL_N_ROIS_EXPECTED}).\")\n",
    "            logger.error(error_msg)\n",
    "            return None, error_msg, False\n",
    "\n",
    "        logger.info(f\"S {subject_id}: Successfully loaded and preprocessed. Original TPs: {original_tp_count}, Final Shape for conn: {final_shape_str}\")\n",
    "        return sigs_processed, f\"OK. Original TPs: {original_tp_count}, final shape for conn: {final_shape_str}\", True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unhandled exception during load_and_preprocess for S {subject_id} ({mat_path.name}): {e}\", exc_info=True)\n",
    "        return None, f\"Exception processing {mat_path.name}: {str(e)}\", False\n",
    "\n",
    "# --- 3. Connectivity Calculation Functions ---\n",
    "def fisher_r_to_z(r_matrix: np.ndarray, eps: float = 1e-7) -> np.ndarray:\n",
    "    # ... (código sin cambios) ...\n",
    "    r_clean = np.nan_to_num(r_matrix.astype(np.float32), nan=0.0) \n",
    "    r_clipped = np.clip(r_clean, -1.0 + eps, 1.0 - eps)\n",
    "    z_matrix = np.arctanh(r_clipped)\n",
    "    np.fill_diagonal(z_matrix, 0.0) \n",
    "    return z_matrix.astype(np.float32)\n",
    "\n",
    "def calculate_pearson_full_fisher_z(ts_subject: np.ndarray, sid: str) -> Optional[np.ndarray]:\n",
    "    # ... (código sin cambios) ...\n",
    "    if ts_subject.shape[0] < 2:\n",
    "        logger.warning(f\"Pearson_Full_FisherZ (S {sid}): Insufficient timepoints ({ts_subject.shape[0]} < 2).\")\n",
    "        return None\n",
    "    try:\n",
    "        corr_matrix = np.corrcoef(ts_subject, rowvar=False).astype(np.float32)\n",
    "        if corr_matrix.ndim == 0: \n",
    "            logger.warning(f\"Pearson_Full_FisherZ (S {sid}): Correlation resulted in a scalar. Input shape: {ts_subject.shape}.\")\n",
    "            num_rois = ts_subject.shape[1]\n",
    "            return np.zeros((num_rois, num_rois), dtype=np.float32) if num_rois > 0 else None\n",
    "        \n",
    "        z_transformed_matrix = fisher_r_to_z(corr_matrix)\n",
    "        logger.info(f\"Pearson_Full_FisherZ (S {sid}): Successfully calculated.\")\n",
    "        return z_transformed_matrix\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating Pearson_Full_FisherZ for S {sid}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def calculate_pearson_omst(ts_subject: np.ndarray, sid: str) -> Optional[np.ndarray]:\n",
    "    if not OMST_PYTHON_LOADED or orthogonal_minimum_spanning_tree is None:\n",
    "        logger.error(f\"Pearson_OMST_GCE (S {sid}): Dyconnmap OMST function not available. Cannot calculate.\")\n",
    "        return None \n",
    "    \n",
    "    if ts_subject.shape[0] < 2: \n",
    "        logger.warning(f\"Pearson_OMST_GCE (S {sid}): Insufficient timepoints ({ts_subject.shape[0]} < 2).\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Supresión de warnings de división por cero específicamente para esta función\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in divide\", category=RuntimeWarning)\n",
    "            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in divide\", category=RuntimeWarning) # Para 1.0/inf\n",
    "            \n",
    "            corr_matrix = np.corrcoef(ts_subject, rowvar=False).astype(np.float32)\n",
    "            \n",
    "            if corr_matrix.ndim == 0: \n",
    "                logger.warning(f\"Pearson_OMST_GCE (S {sid}): Correlation resulted in a scalar. Input shape: {ts_subject.shape}. Returning zero matrix.\")\n",
    "                num_rois = ts_subject.shape[1]\n",
    "                return np.zeros((num_rois, num_rois), dtype=np.float32) if num_rois > 0 else None\n",
    "                \n",
    "            z_transformed_matrix = fisher_r_to_z(corr_matrix)\n",
    "            weights_for_omst_gce = np.abs(z_transformed_matrix) \n",
    "            np.fill_diagonal(weights_for_omst_gce, 0.0) \n",
    "\n",
    "            if np.all(np.isclose(weights_for_omst_gce, 0)):\n",
    "                 logger.warning(f\"Pearson_OMST_GCE (S {sid}): All input weights for OMST GCE are zero. Returning zero matrix.\")\n",
    "                 return weights_for_omst_gce.astype(np.float32)\n",
    "                 \n",
    "            logger.info(f\"S {sid}: Calling dyconnmap.threshold_omst_global_cost_efficiency with weights shape {weights_for_omst_gce.shape}\")\n",
    "            \n",
    "            omst_outputs = orthogonal_minimum_spanning_tree(weights_for_omst_gce, n_msts=None) \n",
    "            \n",
    "            if isinstance(omst_outputs, tuple) and len(omst_outputs) >= 2:\n",
    "                omst_matrix = np.asarray(omst_outputs[1]) \n",
    "                logger.debug(f\"S {sid}: dyconnmap.threshold_omst_global_cost_efficiency returned multiple outputs. Using the second one (CIJtree) as omst_matrix.\")\n",
    "            else:\n",
    "                logger.error(f\"S {sid}: dyconnmap.threshold_omst_global_cost_efficiency returned an unexpected type or insufficient outputs: {type(omst_outputs)}. Cannot extract OMST matrix.\")\n",
    "                return None\n",
    "\n",
    "            if not isinstance(omst_matrix, np.ndarray): \n",
    "                logger.error(f\"S {sid}: Extracted omst_matrix is not a numpy array (type: {type(omst_matrix)}). Cannot proceed.\")\n",
    "                return None\n",
    "\n",
    "            np.fill_diagonal(omst_matrix, 0.0) \n",
    "            \n",
    "            logger.info(f\"Pearson_OMST_GCE (S {sid}): Successfully calculated using dyconnmap. Matrix density: {np.count_nonzero(omst_matrix) / omst_matrix.size:.4f}\")\n",
    "            return omst_matrix.astype(np.float32)\n",
    "\n",
    "    except AttributeError as ae:\n",
    "        if 'from_numpy_matrix' in str(ae).lower() or 'from_numpy_array' in str(ae).lower(): \n",
    "            logger.error(f\"Error calculating Pearson_OMST_GCE (dyconnmap) for S {sid}: NetworkX version incompatibility. \"\n",
    "                         f\"Dyconnmap (v1.0.4) may be using a deprecated NetworkX function. \"\n",
    "                         f\"Your NetworkX version: {nx.__version__}. Consider using NetworkX 2.x (e.g., 'pip install networkx==2.8.8'). Original error: {ae}\", exc_info=False) \n",
    "        else:\n",
    "            logger.error(f\"AttributeError calculating Pearson_OMST_GCE (dyconnmap) for S {sid}: {ae}\", exc_info=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating Pearson_OMST_GCE (dyconnmap) connectivity for S {sid}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def calculate_mi_knn_connectivity(ts_subject: np.ndarray, n_neighbors_val: int, sid: str) -> Optional[np.ndarray]:\n",
    "    # ... (código con MI simétrica ya implementado) ...\n",
    "    n_tp, n_rois = ts_subject.shape\n",
    "    if n_tp == 0: \n",
    "        logger.warning(f\"MI_KNN (S {sid}): 0 Timepoints provided. Cannot calculate MI.\")\n",
    "        return None\n",
    "    if n_tp <= n_neighbors_val: \n",
    "        logger.warning(f\"MI_KNN (S {sid}): Timepoints ({n_tp}) <= n_neighbors ({n_neighbors_val}). Skipping MI calculation.\")\n",
    "        return None\n",
    "        \n",
    "    mi_matrix = np.zeros((n_rois, n_rois), dtype=np.float32)\n",
    "    for i in range(n_rois):\n",
    "        for j in range(i + 1, n_rois): \n",
    "            try:\n",
    "                X_i_reshaped = ts_subject[:, i].reshape(-1, 1)\n",
    "                y_j = ts_subject[:, j]\n",
    "                \n",
    "                mi_val_ij = mutual_info_regression(X_i_reshaped, y_j, n_neighbors=n_neighbors_val, random_state=42, discrete_features=False)[0]\n",
    "                \n",
    "                X_j_reshaped = ts_subject[:, j].reshape(-1, 1)\n",
    "                y_i = ts_subject[:, i]\n",
    "                mi_val_ji = mutual_info_regression(X_j_reshaped, y_i, n_neighbors=n_neighbors_val, random_state=42, discrete_features=False)[0]\n",
    "                \n",
    "                mi_matrix[i, j] = mi_matrix[j, i] = (mi_val_ij + mi_val_ji) / 2.0\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"MI_KNN (S {sid}), error calculating for ROI pair ({i},{j}): {e}\", exc_info=False)\n",
    "                mi_matrix[i, j] = mi_matrix[j, i] = 0.0 \n",
    "    logger.info(f\"MI_KNN_Symmetric (S {sid}): Successfully calculated.\")\n",
    "    return mi_matrix\n",
    "\n",
    "def calculate_custom_dfc_abs_diff_mean(ts_subject: np.ndarray, win_points_val: int, step_val: int, sid: str) -> Optional[np.ndarray]:\n",
    "    # ... (código sin cambios) ...\n",
    "    n_tp, n_rois = ts_subject.shape\n",
    "    if n_tp < win_points_val: \n",
    "        logger.warning(f\"dFC_AbsDiffMean (S {sid}): Timepoints ({n_tp}) < window length ({win_points_val}). Skipping.\")\n",
    "        return None\n",
    "        \n",
    "    num_windows = (n_tp - win_points_val) // step_val + 1\n",
    "    if num_windows < 2: \n",
    "        logger.warning(f\"dFC_AbsDiffMean (S {sid}): Fewer than 2 windows ({num_windows}) can be formed. Skipping.\")\n",
    "        return None\n",
    "        \n",
    "    sum_abs_diff_matrix = np.zeros((n_rois, n_rois), dtype=np.float64) \n",
    "    n_diffs_calculated = 0\n",
    "    prev_corr_matrix_abs: Optional[np.ndarray] = None\n",
    "    \n",
    "    for idx in range(num_windows):\n",
    "        start_idx = idx * step_val\n",
    "        end_idx = start_idx + win_points_val\n",
    "        window_ts = ts_subject[start_idx:end_idx, :]\n",
    "        \n",
    "        if window_ts.shape[0] < 2: continue \n",
    "            \n",
    "        try:\n",
    "            corr_matrix_window = np.corrcoef(window_ts, rowvar=False)\n",
    "            if corr_matrix_window.ndim < 2 or corr_matrix_window.shape != (n_rois, n_rois):\n",
    "                logger.warning(f\"dFC_AbsDiffMean (S {sid}), Window {idx}: corrcoef returned unexpected shape {corr_matrix_window.shape}. Using zeros for this window.\")\n",
    "                corr_matrix_window = np.full((n_rois, n_rois), 0.0, dtype=np.float32)\n",
    "            else:\n",
    "                corr_matrix_window = np.nan_to_num(corr_matrix_window.astype(np.float32), nan=0.0) \n",
    "            \n",
    "            current_corr_matrix_abs = np.abs(corr_matrix_window)\n",
    "            np.fill_diagonal(current_corr_matrix_abs, 0) \n",
    "            \n",
    "            if prev_corr_matrix_abs is not None:\n",
    "                sum_abs_diff_matrix += np.abs(current_corr_matrix_abs - prev_corr_matrix_abs)\n",
    "                n_diffs_calculated += 1\n",
    "            prev_corr_matrix_abs = current_corr_matrix_abs\n",
    "        except Exception as e: \n",
    "            logger.error(f\"dFC_AbsDiffMean (S {sid}), Window {idx}: Error calculating/processing correlation: {e}\")\n",
    "            \n",
    "    if n_diffs_calculated == 0: \n",
    "        logger.warning(f\"dFC_AbsDiffMean (S {sid}): No valid differences between windowed correlations were calculated. Returning None.\")\n",
    "        return None\n",
    "        \n",
    "    mean_abs_diff_matrix = (sum_abs_diff_matrix / n_diffs_calculated).astype(np.float32)\n",
    "    np.fill_diagonal(mean_abs_diff_matrix, 0) \n",
    "    return mean_abs_diff_matrix\n",
    "\n",
    "def calculate_elasticnet_var_connectivity_matrix(ts: np.ndarray, max_lag_val: int, sid: str) -> Optional[np.ndarray]: # MODIFICADO: Nombre de función\n",
    "    n_tp, n_rois = ts.shape\n",
    "    p = max_lag_val \n",
    "    \n",
    "    if n_tp <= p: \n",
    "        logger.warning(f\"ElasticNetVAR (S {sid}): Timepoints ({n_tp}) <= max_lag ({p}). Skipping VAR.\")\n",
    "        return None\n",
    "    \n",
    "    if n_tp < p * n_rois / 5 : \n",
    "        logger.warning(f\"ElasticNetVAR (S {sid}): Timepoints ({n_tp}) are very low for ROIs*lag ({n_rois*p}). Model might be unstable or overfit.\")\n",
    "    elif n_tp < p * n_rois: \n",
    "        logger.info(f\"ElasticNetVAR (S {sid}): Timepoints ({n_tp}) < ROIs*lag ({n_rois*p}). Relying on ElasticNet for regularization.\")\n",
    "\n",
    "    Y_data = ts[p:, :].astype(np.float32)\n",
    "    X_data_list = []\n",
    "    for lag_idx in range(1, p + 1):\n",
    "        X_data_list.append(ts[p-lag_idx : n_tp-lag_idx, :])\n",
    "    X_data = np.hstack(X_data_list).astype(np.float32)\n",
    "    \n",
    "    scaler_X, scaler_Y = StandardScaler(), StandardScaler()\n",
    "    try:\n",
    "        X_scaled = scaler_X.fit_transform(X_data)\n",
    "        Y_scaled = scaler_Y.fit_transform(Y_data)\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"ElasticNetVAR (S {sid}): Scaling failed (e.g., constant data) - {e}. Skipping.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        model = MultiTaskElasticNetCV( # MODIFICADO: Usa MultiTaskElasticNetCV\n",
    "            l1_ratio=ENET_L1_RATIOS, # MODIFICADO: Añadido l1_ratios\n",
    "            alphas=ENET_ALPHAS,      # MODIFICADO: Usa ENET_ALPHAS\n",
    "            cv=5, \n",
    "            n_jobs=1, \n",
    "            max_iter=ENET_MAX_ITER,  # MODIFICADO: Usa ENET_MAX_ITER\n",
    "            tol=ENET_TOL,            # MODIFICADO: Usa ENET_TOL\n",
    "            selection=ENET_SELECTION,  # MODIFICADO: Usa ENET_SELECTION\n",
    "            random_state=42, \n",
    "            verbose=False\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "            model.fit(X_scaled, Y_scaled)\n",
    "        logger.info(f\"S {sid}: ElasticNetCV optimal alpha: {model.alpha_:.6f}, optimal l1_ratio: {model.l1_ratio_:.4f}\") # Log el alpha y l1_ratio óptimos\n",
    "            \n",
    "    except NameError as ne: \n",
    "        if 'ConvergenceWarning' in str(ne): \n",
    "            logger.error(f\"ElasticNetVAR (S {sid}): Model fitting failed – 'ConvergenceWarning' is not defined. Please import it from sklearn.exceptions.\")\n",
    "            return None\n",
    "        else: \n",
    "            logger.error(f\"ElasticNetVAR (S {sid}): Model fitting failed due to NameError – {ne}\", exc_info=False)\n",
    "            return None\n",
    "    except Exception as e: \n",
    "        logger.error(f\"ElasticNetVAR (S {sid}): Model fitting failed – {e}\", exc_info=False)\n",
    "        return None\n",
    "        \n",
    "    var_coeffs_scaled = model.coef_ \n",
    "    \n",
    "    influence_matrix = np.zeros((n_rois, n_rois), dtype=np.float32)\n",
    "    for target_roi_idx in range(n_rois): \n",
    "        for source_roi_idx in range(n_rois): \n",
    "            coeffs_for_pair_at_lags = [\n",
    "                var_coeffs_scaled[target_roi_idx, source_roi_idx + (lag_k * n_rois)] \n",
    "                for lag_k in range(p)\n",
    "            ]\n",
    "            influence_matrix[target_roi_idx, source_roi_idx] = np.sum(np.abs(coeffs_for_pair_at_lags))\n",
    "            \n",
    "    np.fill_diagonal(influence_matrix, 0.0) \n",
    "    return influence_matrix\n",
    "\n",
    "# --- 4. Function to Calculate All Connectivity Modalities ---\n",
    "def calculate_all_connectivity_modalities_for_subject(\n",
    "    subject_id: str, subject_ts_data: np.ndarray,\n",
    "    n_neighbors_mi_param: int,\n",
    "    dfc_win_points_param: int, dfc_step_param: int,\n",
    "    var_lag_param: int # MODIFICADO: Renombrado\n",
    ") -> Dict[str, Any]:\n",
    "    # ... (código con la lógica de fallback ya implementada) ...\n",
    "    matrices: Dict[str, Optional[np.ndarray]] = {name: None for name in CONNECTIVITY_CHANNEL_NAMES}\n",
    "    errors_in_calculation: Dict[str, str] = {}\n",
    "    timings: Dict[str, float] = {}\n",
    "\n",
    "    for channel_name in CONNECTIVITY_CHANNEL_NAMES:\n",
    "        logger.info(f\"Calculating {channel_name} for S {subject_id} (TS shape: {subject_ts_data.shape})...\")\n",
    "        start_time_channel = time.time()\n",
    "        matrix_result: Optional[np.ndarray] = None\n",
    "        error_msg: Optional[str] = None\n",
    "\n",
    "        try:\n",
    "            if channel_name == PEARSON_OMST_CHANNEL_NAME_PRIMARY: \n",
    "                if OMST_PYTHON_LOADED and orthogonal_minimum_spanning_tree is not None:\n",
    "                    matrix_result = calculate_pearson_omst(subject_ts_data, subject_id)\n",
    "                    if matrix_result is None: \n",
    "                        error_msg = f\"Primary OMST GCE calculation failed for S {subject_id}. Check logs for specific error (e.g. NetworkX).\"\n",
    "                        logger.error(error_msg) \n",
    "                else: \n",
    "                    error_msg = f\"OMST function not loaded, cannot calculate '{PEARSON_OMST_CHANNEL_NAME_PRIMARY}' for S {subject_id}.\"\n",
    "                    logger.error(error_msg)\n",
    "            elif channel_name == PEARSON_OMST_FALLBACK_NAME: \n",
    "                 matrix_result = calculate_pearson_full_fisher_z(subject_ts_data, subject_id)\n",
    "            elif channel_name == \"MI_KNN_Symmetric\" and USE_MI_CHANNEL_FOR_THESIS: \n",
    "                matrix_result = calculate_mi_knn_connectivity(subject_ts_data, n_neighbors_mi_param, subject_id)\n",
    "            elif channel_name == \"dFC_AbsDiffMean\" and USE_DFC_CHANNEL_FOR_THESIS:\n",
    "                matrix_result = calculate_custom_dfc_abs_diff_mean(subject_ts_data, dfc_win_points_param, dfc_step_param, subject_id)\n",
    "            elif channel_name.startswith(\"ElasticNet_VAR\") and USE_VAR_CHANNEL_FOR_THESIS: # MODIFICADO: Chequea por ElasticNet\n",
    "                matrix_result = calculate_elasticnet_var_connectivity_matrix(subject_ts_data, var_lag_param, subject_id) # MODIFICADO: Llama a la función ElasticNet\n",
    "            \n",
    "            if matrix_result is None and error_msg is None and channel_name in CONNECTIVITY_CHANNEL_NAMES :\n",
    "                error_msg = f\"'{channel_name}' was in CONNECTIVITY_CHANNEL_NAMES but not calculated or its function returned None.\"\n",
    "                logger.warning(error_msg)\n",
    "\n",
    "        except Exception as e: \n",
    "            error_msg = str(e)\n",
    "            logger.error(f\"Unexpected error while attempting to calculate {channel_name} for S {subject_id}: {e}\", exc_info=True)\n",
    "        \n",
    "        matrices[channel_name] = matrix_result\n",
    "        if error_msg and channel_name not in errors_in_calculation : \n",
    "            errors_in_calculation[channel_name] = error_msg\n",
    "        \n",
    "        timings[f\"{channel_name}_time_sec\"] = time.time() - start_time_channel\n",
    "        if matrix_result is not None:\n",
    "            logger.info(f\"{channel_name} for S {subject_id} calculated. Shape: {matrix_result.shape}. Took {timings[f'{channel_name}_time_sec']:.2f}s.\")\n",
    "        elif channel_name in CONNECTIVITY_CHANNEL_NAMES: \n",
    "            logger.warning(f\"{channel_name} for S {subject_id} failed or returned None. Took {timings[f'{channel_name}_time_sec']:.2f}s. Error: {errors_in_calculation.get(channel_name, 'N/A')}\")\n",
    "            \n",
    "    num_modalities_expected = len(CONNECTIVITY_CHANNEL_NAMES)\n",
    "    num_successful = sum(1 for mat_name in CONNECTIVITY_CHANNEL_NAMES if matrices.get(mat_name) is not None)\n",
    "    \n",
    "    if num_successful < num_modalities_expected:\n",
    "        logger.warning(f\"Connectivity for S {subject_id}: {num_successful}/{num_modalities_expected} selected modalities computed. Errors: {errors_in_calculation}\")\n",
    "    else:\n",
    "        logger.info(f\"Connectivity for S {subject_id}: All {num_successful}/{num_modalities_expected} selected modalities computed successfully.\")\n",
    "\n",
    "    return {\"matrices\": matrices, \"errors_conn_calc\": errors_in_calculation, \"timings_conn_calc\": timings}\n",
    "\n",
    "\n",
    "# --- 5. Per-Subject Processing Pipeline (for Multiprocessing) ---\n",
    "def process_single_subject_pipeline(subject_row_tuple: Tuple[int, pd.Series]) -> Dict[str, Any]:\n",
    "    # ... (código sin cambios significativos, ya maneja los resultados de calculate_all_connectivity_modalities_for_subject) ...\n",
    "    idx, subject_row = subject_row_tuple\n",
    "    subject_id = str(subject_row['SubjectID']).strip()\n",
    "    process = psutil.Process(os.getpid()) \n",
    "    ram_initial_mb = process.memory_info().rss / (1024**2)\n",
    "    \n",
    "    result: Dict[str, Any] = {\n",
    "        \"id\": subject_id, \n",
    "        \"status_preprocessing\": \"PENDING\", \"detail_preprocessing\": \"\",\n",
    "        \"status_connectivity_calc\": \"NOT_ATTEMPTED\", \"errors_connectivity_calc\": {},\n",
    "        \"timings_connectivity_calc_sec\": {}, \"path_saved_tensor\": None,\n",
    "        \"status_overall\": \"PENDING\",\n",
    "        \"ram_usage_mb_initial\": ram_initial_mb, \"ram_usage_mb_final\": -1.0\n",
    "    }\n",
    "    series_data: Optional[np.ndarray] = None \n",
    "\n",
    "    try:\n",
    "        series_data, detail_msg_preproc, success_preproc = load_and_preprocess_single_subject_series(\n",
    "            subject_id, \n",
    "            TARGET_LEN_TS,\n",
    "            ROI_SIGNALS_DIR_PATH_AAL3, ROI_FILENAME_TEMPLATE, POSSIBLE_ROI_KEYS,\n",
    "            VAR_MAX_LAG, TR_SECONDS, LOW_CUT_HZ, HIGH_CUT_HZ, FILTER_ORDER, # MODIFICADO: Renombrado\n",
    "            APPLY_HRF_DECONVOLUTION, HRF_MODEL,\n",
    "            taper_alpha_val=TAPER_ALPHA \n",
    "        )\n",
    "        result[\"status_preprocessing\"] = \"SUCCESS\" if success_preproc else \"FAILED\"\n",
    "        result[\"detail_preprocessing\"] = detail_msg_preproc\n",
    "        \n",
    "        if not success_preproc or series_data is None:\n",
    "            result[\"status_overall\"] = \"PREPROCESSING_FAILED\"\n",
    "            return result \n",
    "\n",
    "        connectivity_results = calculate_all_connectivity_modalities_for_subject(\n",
    "            subject_id, series_data, N_NEIGHBORS_MI,\n",
    "            DFC_WIN_POINTS, DFC_STEP, VAR_MAX_LAG # MODIFICADO: Renombrado\n",
    "        )\n",
    "        del series_data; series_data = None; gc.collect() \n",
    "        \n",
    "        calculated_matrices_dict = connectivity_results[\"matrices\"]\n",
    "        result[\"errors_connectivity_calc\"] = connectivity_results[\"errors_conn_calc\"]\n",
    "        result[\"timings_connectivity_calc_sec\"] = connectivity_results.get(\"timings_conn_calc\", {})\n",
    "\n",
    "        all_modalities_valid_and_present = True\n",
    "        final_matrices_to_stack_list = []\n",
    "        \n",
    "        current_expected_rois_for_matrices = FINAL_N_ROIS_EXPECTED if FINAL_N_ROIS_EXPECTED is not None else N_ROIS_EXPECTED \n",
    "        if current_expected_rois_for_matrices is None: \n",
    "            logger.critical(f\"S {subject_id}: CRITICAL - current_expected_rois_for_matrices is None. Cannot validate matrix shapes.\")\n",
    "            result[\"status_overall\"] = \"FAILURE_CRITICAL_ROI_COUNT_UNSET\"\n",
    "            result[\"status_connectivity_calc\"] = \"FAILURE_CRITICAL_ROI_COUNT_UNSET\"\n",
    "            return result\n",
    "\n",
    "        expected_matrix_shape = (current_expected_rois_for_matrices, current_expected_rois_for_matrices)\n",
    "\n",
    "        for channel_name in CONNECTIVITY_CHANNEL_NAMES: \n",
    "            matrix = calculated_matrices_dict.get(channel_name)\n",
    "            if matrix is None:\n",
    "                all_modalities_valid_and_present = False\n",
    "                err_msg = f\"Modality '{channel_name}' result is None (check calculation logs for S {subject_id}).\" \n",
    "                logger.error(f\"S {subject_id}: {err_msg}\")\n",
    "                if channel_name not in result[\"errors_connectivity_calc\"]:\n",
    "                    result[\"errors_connectivity_calc\"][channel_name] = err_msg\n",
    "                break \n",
    "            elif matrix.shape != expected_matrix_shape:\n",
    "                all_modalities_valid_and_present = False\n",
    "                err_msg = f\"Modality '{channel_name}' shape {matrix.shape} != expected {expected_matrix_shape}.\"\n",
    "                logger.error(f\"S {subject_id}: {err_msg}\")\n",
    "                result[\"errors_connectivity_calc\"][channel_name] = result[\"errors_connectivity_calc\"].get(channel_name, \"\") + \" | \" + err_msg\n",
    "                break \n",
    "            else:\n",
    "                final_matrices_to_stack_list.append(matrix)\n",
    "        \n",
    "        if all_modalities_valid_and_present and len(final_matrices_to_stack_list) == N_CHANNELS: \n",
    "            result[\"status_connectivity_calc\"] = \"SUCCESS_ALL_MODALITIES_VALID\"\n",
    "            try:\n",
    "                subject_tensor = np.stack(final_matrices_to_stack_list, axis=0).astype(np.float32)\n",
    "                del final_matrices_to_stack_list, calculated_matrices_dict; gc.collect() \n",
    "                \n",
    "                output_dir_individual_tensors = BASE_PATH_AAL3 / OUTPUT_CONNECTIVITY_DIR_NAME / \"individual_subject_tensors\"\n",
    "                output_dir_individual_tensors.mkdir(parents=True, exist_ok=True) \n",
    "                \n",
    "                output_path = output_dir_individual_tensors / f\"tensor_{N_CHANNELS}ch_{current_expected_rois_for_matrices}rois_{subject_id}.npz\"\n",
    "                np.savez_compressed(output_path, \n",
    "                                    tensor_data=subject_tensor, \n",
    "                                    subject_id=subject_id, \n",
    "                                    channel_names=np.array(CONNECTIVITY_CHANNEL_NAMES, dtype=str),\n",
    "                                    rois_count = current_expected_rois_for_matrices,\n",
    "                                    target_len_ts = TARGET_LEN_TS\n",
    "                                    )\n",
    "                result[\"path_saved_tensor\"] = str(output_path)\n",
    "                result[\"status_overall\"] = \"SUCCESS_ALL_PROCESSED_AND_SAVED\"\n",
    "                logger.info(f\"S {subject_id}: Successfully processed. Tensor saved to {output_path.name}\")\n",
    "                del subject_tensor; gc.collect() \n",
    "            except Exception as e_save:\n",
    "                logger.error(f\"Error saving tensor for S {subject_id}: {e_save}\", exc_info=True)\n",
    "                result[\"errors_connectivity_calc\"][\"save_error\"] = str(e_save)\n",
    "                result[\"status_overall\"] = \"FAILURE_DURING_TENSOR_SAVING\"\n",
    "                result[\"status_connectivity_calc\"] = \"FAILURE_DURING_SAVING\"\n",
    "        else:\n",
    "            result[\"status_overall\"] = \"FAILURE_IN_CONNECTIVITY_CALC_OR_VALIDATION\"\n",
    "            result[\"status_connectivity_calc\"] = \"FAILURE_MISSING_OR_INVALID_MODALITIES\"\n",
    "            if not all_modalities_valid_and_present: \n",
    "                logger.error(f\"S {subject_id}: Not all connectivity modalities were valid or present. Tensor not saved. Errors: {result['errors_connectivity_calc']}\")\n",
    "    \n",
    "    except Exception as e_pipeline: \n",
    "        logger.critical(f\"CRITICAL UNHANDLED EXCEPTION for S {subject_id} in pipeline: {e_pipeline}\", exc_info=True)\n",
    "        result[\"status_overall\"] = \"CRITICAL_PIPELINE_EXCEPTION\"\n",
    "        result[\"detail_preprocessing\"] = result.get(\"detail_preprocessing\",\"\") + \" | Pipeline Exc: \" + str(e_pipeline)\n",
    "        result[\"errors_connectivity_calc\"][\"pipeline_exception\"] = str(e_pipeline)\n",
    "    \n",
    "    finally: \n",
    "        if series_data is not None: del series_data; gc.collect() \n",
    "        result[\"ram_usage_mb_final\"] = process.memory_info().rss / (1024**2)\n",
    "    return result\n",
    "\n",
    "# --- 6. Main Script Execution Flow ---\n",
    "def main():\n",
    "    # --- Diagnóstico de Versión de NetworkX ---\n",
    "    try:\n",
    "        import networkx as nx_runtime\n",
    "        logger.info(f\"RUNTIME NetworkX version being used: {nx_runtime.__version__}\")\n",
    "    except ImportError:\n",
    "        logger.error(\"RUNTIME: NetworkX is not installed or importable.\")\n",
    "\n",
    "    # --- Fijar semilla para reproducibilidad ---\n",
    "    np.random.seed(42)\n",
    "\n",
    "    script_start_time = time.time()\n",
    "    main_process_info = psutil.Process(os.getpid())\n",
    "    logger.info(f\"Main process RAM at start: {main_process_info.memory_info().rss / (1024**2):.2f} MB\")\n",
    "    logger.info(f\"--- Starting fMRI Connectivity Pipeline (Version for Doctoral Thesis with Dyconnmap v6.5.3_ElasticNetRelaxed) ---\") \n",
    "    \n",
    "    logger.info(f\"--- Final Expected ROIs for Connectivity Matrices: {N_ROIS_EXPECTED} (should be 131) ---\")\n",
    "    logger.info(f\"--- Target Homogenized Time Series Length: {TARGET_LEN_TS} ---\")\n",
    "    logger.info(f\"--- Output Directory Name: {OUTPUT_CONNECTIVITY_DIR_NAME} ---\")\n",
    "    logger.info(f\"--- Selected Connectivity Channels for VAE: {CONNECTIVITY_CHANNEL_NAMES} ({N_CHANNELS} channels) ---\")\n",
    "    if not (OMST_PYTHON_LOADED and orthogonal_minimum_spanning_tree is not None):\n",
    "        logger.warning(f\"Note: OMST from dyconnmap could not be loaded. '{PEARSON_OMST_FALLBACK_NAME}' will be used instead of '{PEARSON_OMST_CHANNEL_NAME_PRIMARY}'.\")\n",
    "\n",
    "\n",
    "    if not BASE_PATH_AAL3.exists() or not ROI_SIGNALS_DIR_PATH_AAL3.exists():\n",
    "        logger.critical(f\"CRITICAL: Base AAL3 path ({BASE_PATH_AAL3}) or ROI signals directory ({ROI_SIGNALS_DIR_PATH_AAL3}) not found. Aborting.\")\n",
    "        return\n",
    "\n",
    "    subject_metadata_df = load_metadata(SUBJECT_METADATA_CSV_PATH, QC_REPORT_CSV_PATH)\n",
    "    if subject_metadata_df is None or subject_metadata_df.empty:\n",
    "        logger.critical(\"Metadata loading failed or no subjects passed QC to process. Aborting.\")\n",
    "        return\n",
    "\n",
    "    output_main_directory = BASE_PATH_AAL3 / OUTPUT_CONNECTIVITY_DIR_NAME\n",
    "    output_individual_tensors_dir = output_main_directory / \"individual_subject_tensors\" \n",
    "    try:\n",
    "        output_main_directory.mkdir(parents=True, exist_ok=True)\n",
    "        output_individual_tensors_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Main output directory created/exists: {output_main_directory}\")\n",
    "    except OSError as e:\n",
    "        logger.critical(f\"Could not create output directories: {e}. Aborting.\"); return\n",
    "\n",
    "    total_cpu_cores = multiprocessing.cpu_count()\n",
    "    MAX_WORKERS = max(1, total_cpu_cores // 2 if total_cpu_cores > 2 else 1) \n",
    "    logger.info(f\"Total CPU cores available: {total_cpu_cores}. Using MAX_WORKERS = {MAX_WORKERS}.\")\n",
    "    available_ram_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    logger.warning(f\"Available system RAM at start of parallel processing: {available_ram_gb:.2f} GB. Monitor usage closely.\")\n",
    "\n",
    "    subject_rows_to_process = list(subject_metadata_df.iterrows()) \n",
    "    num_subjects_to_process = len(subject_rows_to_process)\n",
    "    if num_subjects_to_process == 0: \n",
    "        logger.critical(\"No subjects to process after metadata loading and QC filtering. Aborting.\")\n",
    "        return\n",
    "    logger.info(f\"Starting parallel processing for {num_subjects_to_process} subjects.\")\n",
    "    \n",
    "    all_subject_results_list = [] \n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_subject_id_map = {\n",
    "            executor.submit(process_single_subject_pipeline, subject_tuple): str(subject_tuple[1]['SubjectID']).strip()\n",
    "            for subject_tuple in subject_rows_to_process\n",
    "        }\n",
    "        for future in tqdm(as_completed(future_to_subject_id_map), total=num_subjects_to_process, desc=\"Processing Subjects\"):\n",
    "            subject_id_for_log = future_to_subject_id_map[future]\n",
    "            try:\n",
    "                subject_result = future.result() \n",
    "                all_subject_results_list.append(subject_result)\n",
    "            except Exception as exc: \n",
    "                logger.critical(f\"CRITICAL WORKER EXCEPTION for S {subject_id_for_log}: {exc}\", exc_info=True)\n",
    "                all_subject_results_list.append({ \n",
    "                    \"id\": subject_id_for_log, \"status_overall\": \"CRITICAL_WORKER_EXCEPTION\",\n",
    "                    \"detail_preprocessing\": f\"Worker process crashed: {str(exc)}\", \n",
    "                    \"errors_connectivity_calc\": {\"worker_exception\": str(exc)}\n",
    "                })\n",
    "\n",
    "    processing_log_df = pd.DataFrame(all_subject_results_list)\n",
    "    log_file_path = output_main_directory / f\"pipeline_log_{output_main_directory.name}.csv\"\n",
    "    try: \n",
    "        processing_log_df.to_csv(log_file_path, index=False)\n",
    "        logger.info(f\"Detailed processing log saved to: {log_file_path}\")\n",
    "    except Exception as e_log_save: \n",
    "        logger.error(f\"Failed to save detailed processing log: {e_log_save}\")\n",
    "\n",
    "    successful_subject_entries_list = [\n",
    "        res for res in all_subject_results_list\n",
    "        if res.get(\"status_overall\") == \"SUCCESS_ALL_PROCESSED_AND_SAVED\" and \\\n",
    "           res.get(\"path_saved_tensor\") and Path(res[\"path_saved_tensor\"]).exists()\n",
    "    ]\n",
    "    num_successful_subjects_for_tensor = len(successful_subject_entries_list)\n",
    "    \n",
    "    logger.info(f\"--- Overall Processing Summary ---\")\n",
    "    logger.info(f\"Total subjects attempted: {num_subjects_to_process}\")\n",
    "    logger.info(f\"Successfully processed and individual tensors saved: {num_successful_subjects_for_tensor}\")\n",
    "    if num_successful_subjects_for_tensor < num_subjects_to_process:\n",
    "        num_failed = num_subjects_to_process - num_successful_subjects_for_tensor\n",
    "        logger.warning(f\"{num_failed} subjects failed at some stage. Check the detailed log: {log_file_path}\")\n",
    "\n",
    "    if num_successful_subjects_for_tensor > 0:\n",
    "        logger.info(f\"Attempting to assemble global tensor for {num_successful_subjects_for_tensor} successfully processed subjects.\")\n",
    "        global_conn_tensor_list = []\n",
    "        final_subject_ids_in_tensor = []\n",
    "        \n",
    "        current_expected_rois_for_assembly = FINAL_N_ROIS_EXPECTED if FINAL_N_ROIS_EXPECTED is not None else N_ROIS_EXPECTED\n",
    "        if current_expected_rois_for_assembly is None:\n",
    "            logger.critical(\"Cannot assemble global tensor: FINAL_N_ROIS_EXPECTED is None.\")\n",
    "        else:\n",
    "            try:\n",
    "                for s_entry in tqdm(successful_subject_entries_list, desc=\"Assembling Global Tensor\"):\n",
    "                    s_id = s_entry[\"id\"]\n",
    "                    tensor_path_str = s_entry[\"path_saved_tensor\"]\n",
    "                    try:\n",
    "                        with np.load(tensor_path_str) as loaded_npz:\n",
    "                            s_tensor_data = loaded_npz['tensor_data']\n",
    "                            if s_tensor_data.shape == (N_CHANNELS, current_expected_rois_for_assembly, current_expected_rois_for_assembly):\n",
    "                                global_conn_tensor_list.append(s_tensor_data)\n",
    "                                final_subject_ids_in_tensor.append(s_id)\n",
    "                            else: \n",
    "                                logger.error(f\"Tensor for S {s_id} has shape mismatch: {s_tensor_data.shape}. \"\n",
    "                                             f\"Expected: ({N_CHANNELS}, {current_expected_rois_for_assembly}, {current_expected_rois_for_assembly}). Skipping this subject for global tensor.\")\n",
    "                        del s_tensor_data; gc.collect()\n",
    "                    except Exception as e_load_ind_tensor: \n",
    "                        logger.error(f\"Error loading individual tensor for S {s_id} from {tensor_path_str}: {e_load_ind_tensor}. Skipping for global tensor.\")\n",
    "                \n",
    "                if global_conn_tensor_list:\n",
    "                    global_conn_tensor = np.stack(global_conn_tensor_list, axis=0).astype(np.float32)\n",
    "                    del global_conn_tensor_list; gc.collect() \n",
    "                    \n",
    "                    global_tensor_fname = f\"GLOBAL_TENSOR_AAL3_{current_expected_rois_for_assembly}ROIs_{len(final_subject_ids_in_tensor)}subs_{N_CHANNELS}ch_{VAR_MAX_LAG}lag{deconv_str}.npz\" # MODIFICADO\n",
    "                    global_tensor_path = output_main_directory / global_tensor_fname\n",
    "                    \n",
    "                    np.savez_compressed(global_tensor_path, \n",
    "                                        global_tensor_data=global_conn_tensor, \n",
    "                                        subject_ids=np.array(final_subject_ids_in_tensor, dtype=str), \n",
    "                                        channel_names=np.array(CONNECTIVITY_CHANNEL_NAMES, dtype=str),\n",
    "                                        rois_count = current_expected_rois_for_assembly,\n",
    "                                        target_len_ts = TARGET_LEN_TS,\n",
    "                                        tr_seconds = TR_SECONDS,\n",
    "                                        filter_low_hz = LOW_CUT_HZ,\n",
    "                                        filter_high_hz = HIGH_CUT_HZ,\n",
    "                                        hrf_deconvolution_applied = APPLY_HRF_DECONVOLUTION,\n",
    "                                        hrf_model = HRF_MODEL if APPLY_HRF_DECONVOLUTION else \"N/A\"\n",
    "                                        )\n",
    "                    logger.info(f\"Global tensor successfully assembled and saved: {global_tensor_path.name}\")\n",
    "                    logger.info(f\"Global tensor shape: {global_conn_tensor.shape} (Subjects, Channels, ROIs, ROIs)\")\n",
    "                    del global_conn_tensor; gc.collect() \n",
    "                else: \n",
    "                    logger.warning(\"No valid individual tensors were loaded for global assembly. Global tensor not created.\")\n",
    "            except MemoryError: \n",
    "                logger.critical(\"MEMORY ERROR during global tensor assembly. The dataset might be too large to stack in RAM.\")\n",
    "            except Exception as e_global: \n",
    "                logger.critical(f\"An unexpected error occurred during global tensor assembly: {e_global}\", exc_info=True)\n",
    "\n",
    "    total_time_min = (time.time() - script_start_time) / 60\n",
    "    logger.info(f\"--- fMRI Connectivity Pipeline Finished ---\")\n",
    "    logger.info(f\"Total execution time: {total_time_min:.2f} minutes.\")\n",
    "    logger.info(f\"Final main process RAM: {main_process_info.memory_info().rss / (1024**2):.2f} MB\")\n",
    "    logger.info(f\"All outputs, logs, and tensors should be in: {output_main_directory}\")\n",
    "    logger.info(\"Reminder for Thesis: Document all parameters, QC steps, subject selection criteria, and the precise AAL3 ROI definitions used (131 ROIs). Also cite Dyconnmap if used.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.freeze_support() \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
